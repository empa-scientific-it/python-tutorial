{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling With PyTorch\n",
    "\n",
    "## Part 1 – Solutions\n",
    "\n",
    "This notebook is a companion of [Language Modeling with PyTorch – Part 1](./32_language_modeling_1.ipynb) notebook, and contains *proposed* solutions to the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a Trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trigram language model predicts the next character based on the previous **two** characters, unlike a bigram model which only uses one previous character. This additional context should help make more accurate predictions.\n",
    "\n",
    "In this exercise, we will:\n",
    "1. Train a trigram language model that takes **two** characters as input to predict the 3rd one\n",
    "2. Implement this using a neural network approach\n",
    "3. Evaluate the model's performance using loss metrics\n",
    "4. Compare its performance to the bigram model\n",
    "\n",
    "**Key Questions to Address:**\n",
    "1. Did the trigram model improve over the bigram model?\n",
    "2. If yes, by what percentage did it improve?\n",
    "\n",
    "**Intuition:** By considering two previous characters instead of just one, a trigram model captures more context and patterns in the language, which should lead to better predictions and lower loss compared to a bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "words = open(\"data/lm/names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Trigrams\n",
    "\n",
    "Each trigram consists of a sequence of $2$ input characters, followed by $1$ expected output character.\n",
    "The model's task is to predict the output character given the two input characters.\n",
    "\n",
    "To generate these trigrams from our text data, we can extend the sliding-window approach we used for bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:1]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):  # Three char 'sliding-window'\n",
    "        print(ch1, ch2, ch3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Implementation Detail:**\n",
    "\n",
    "Notice how our first trigram is of shape `('.', 'e', 'm')`, and not `('.', '.', 'e')`. This is a deliberate choice.\n",
    "\n",
    "While we could modify the code to produce `('.', '.', 'e')` with `chs = ['.', '.'] + list(w) + ['.', '.']`, having the first two characters as special tokens (both '.') wouldn't provide meaningful context for predicting the next character. Special tokens at the beginning don't contain real linguistic patterns, so starting with `('.')` followed by the first actual character gives our model more useful information.\n",
    "\n",
    "This approach helps avoid confusing the model and reduces wasted computation on inputs that don't reflect natural language patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trigram Occurrences\n",
    "\n",
    "To properly represent and visualize our trigrams, we need to account for all possible character combinations:\n",
    "- We have $26$ letters from the English alphabet\n",
    "- **+1** special character ('.') for word boundaries\n",
    "\n",
    "This gives us a 3D array of dimensions $27\\times 27\\times 27$ to store all possible trigram combinations:\n",
    "- First dimension: the first character in the trigram\n",
    "- Second dimension: the second character in the trigram\n",
    "- Third dimension: the third (predicted) character\n",
    "\n",
    "Each cell in this 3D array will store the count of how many times that specific trigram appears in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)\n",
    "\n",
    "chars = sorted(set(\"\".join(words)))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0  # Special token has position zero\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):  # Three token 'sliding-window'\n",
    "        N[stoi[ch1], stoi[ch2], stoi[ch3]] += 1  # Increment cell in 3D tensor by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a more informative visualization of our trigram counts.\n",
    "We'll visualize the most frequent first characters (up to 3) rather than just the first two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first characters with the most occurrences\n",
    "first_char_counts = N.sum(dim=(1, 2))\n",
    "top_k_indices = torch.topk(first_char_counts, k=3).indices.tolist()\n",
    "\n",
    "# Add special character (.) to always show regardless of frequency\n",
    "if 0 not in top_k_indices:\n",
    "    top_k_indices = [0] + top_k_indices[:2]  # Ensure we include the special token\n",
    "\n",
    "for k in top_k_indices:\n",
    "    # Only show non-zero entries for clarity\n",
    "    nonzero_mask = N[k] > 0\n",
    "\n",
    "    if nonzero_mask.sum() > 0:  # Skip empty slices\n",
    "        plt.figure(figsize=(16, 14))\n",
    "\n",
    "        # Use a perceptually uniform colormap with better contrast\n",
    "        plt.imshow(\n",
    "            N[k],\n",
    "            cmap=\"viridis\",\n",
    "            norm=plt.matplotlib.colors.LogNorm(vmin=0.1, vmax=N[k].max()),\n",
    "        )\n",
    "\n",
    "        plt.colorbar(label=\"Frequency (log scale)\")\n",
    "        plt.title(f\"Trigram Heatmap for First Character: '{itos[k]}'\", fontsize=16)\n",
    "        plt.xlabel(\"Third Character (Predicted)\", fontsize=12)\n",
    "        plt.ylabel(\"Second Character\", fontsize=12)\n",
    "\n",
    "        # Add labels showing both the trigram and its count where non-zero\n",
    "        for i in range(27):\n",
    "            for j in range(27):\n",
    "                if N[k, i, j] > 0:  # Only label non-zero entries\n",
    "                    count = N[k, i, j].item()\n",
    "                    chstr = itos[k] + itos[i] + itos[j]\n",
    "\n",
    "                    # Color text based on background darkness for better readability\n",
    "                    # White text on dark backgrounds, black text on light backgrounds\n",
    "                    norm_val = plt.matplotlib.colors.LogNorm()(count)\n",
    "                    # Use normalized value - lower values are darker in viridis colormap\n",
    "                    # Simple threshold - values below 0.5 get white text, others get black\n",
    "                    text_color = \"white\" if norm_val < 0.5 else \"black\"\n",
    "\n",
    "                    plt.text(\n",
    "                        j,\n",
    "                        i,\n",
    "                        chstr,\n",
    "                        ha=\"center\",\n",
    "                        va=\"bottom\",\n",
    "                        color=text_color,\n",
    "                        fontsize=8,\n",
    "                        fontweight=\"bold\",\n",
    "                    )\n",
    "                    plt.text(\n",
    "                        j, i, count, ha=\"center\", va=\"top\", color=text_color, fontsize=8\n",
    "                    )\n",
    "\n",
    "        # Add grid lines for better readability\n",
    "        plt.grid(False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print summary stats for this first character\n",
    "        total_count = N[k].sum().item()\n",
    "        unique_trigrams = (N[k] > 0).sum().item()\n",
    "        print(\n",
    "            f\"First char '{itos[k]}': {unique_trigrams} unique trigrams out of {total_count} total occurrences\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Interpret the Trigram Heatmaps?**\n",
    "\n",
    "The heatmaps above visualize the frequency of trigram patterns in our dataset:\n",
    "\n",
    "- **X-axis (horizontal)**: The third character in the trigram (the character we're trying to predict)\n",
    "- **Y-axis (vertical)**: The second character in the trigram\n",
    "- **Title**: Shows the first character, which is fixed for each heatmap\n",
    "\n",
    "**Color intensity**: Represents frequency on a logarithmic scale - brighter/more intense colors indicate trigrams that appear more frequently in our dataset.\n",
    "\n",
    "**Labels on cells**: Each cell shows:\n",
    "- The full trigram (at the bottom of the cell)\n",
    "- The count of occurrences (at the top of the cell)\n",
    "\n",
    "**What to look for**:\n",
    "- Bright cells indicate common character combinations\n",
    "- Dark/empty areas show rare or non-existent combinations\n",
    "- Patterns along rows/columns reveal which character sequences occur more frequently in names\n",
    "\n",
    "These visualizations help us understand the statistical patterns our trigram model will learn to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Implementation of the Trigram Model\n",
    "\n",
    "Now let's implement the neural network version of our trigram model. Unlike the bigram model that handled a single input character, the key challenge here is processing two input characters simultaneously to predict the third.\n",
    "\n",
    "The following code prepares our training data for the neural network by:\n",
    "1. Converting each word into a sequence of trigrams\n",
    "2. Extracting input pairs (first two characters) and output targets (third character)\n",
    "3. Converting these into tensor representations the network can process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set of all trigrams\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        xs.append([stoi[ch1], stoi[ch2]])\n",
    "        ys.append(stoi[ch3])\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)  # [196113, 2], [196113]\n",
    "num_x, num_y = xs.nelement() // 2, ys.nelement()\n",
    "print(\"Number of examples\\nx:\", num_x, \"\\ny:\", num_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Representation for the Neural Network\n",
    "\n",
    "For our neural network to process characters, we need to convert them to numerical representations:\n",
    "\n",
    "- Each character is represented by a $27$-dimensional one-hot vector (one position for each of our 26 letters plus 1 special character)\n",
    "- For a trigram, we need to concatenate the one-hot vectors of the two input characters\n",
    "- This creates a $27+27=54$-dimensional input vector for each trigram\n",
    "\n",
    "You can conceptualize this 54-dimensional vector as a \"two-hot\" vector, where exactly two of the 54 dimensions are set to $1$ (one for each input character) and the rest are $0$.\n",
    "\n",
    "Next, we'll initialize our neural network weights and prepare for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "\n",
    "# Random column tensor of (27+27)x27 numbers (requires_grad=True for autograd)\n",
    "W = torch.randn((27 + 27, 27), device=device, generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cycles, using the entire dataset over 200 Epochs, like the bigram model\n",
    "for k in range(200):\n",
    "    # Forward pass\n",
    "    # One-hot encoding, [196113, 2, 27]\n",
    "    xenc = F.one_hot(xs, num_classes=27).float().to(device)\n",
    "    xenc = xenc.view(num_x, -1)  # concatenate the one-hot vectors, [196113, 54]\n",
    "    logits = xenc @ W  # logits, different word for log-counts\n",
    "    counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "\n",
    "    # Normal distribution probabilities (this is y_pred)\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num_x), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    print(f\"Loss @ iteration {k + 1}: {loss}\")\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None  # Make sure all gradients are reset\n",
    "    loss.backward()  # Torch kept track of what this variable is, kinda cool\n",
    "\n",
    "    # Weight update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Comparison\n",
    "\n",
    "**Performance Evaluation:**\n",
    "\n",
    "- The bigram model produced a final loss of $2.462393045425415$\n",
    "- Our trigram model achieves a loss of $2.259373664855957$\n",
    "- This represents an $8.24\\%$ improvement in prediction accuracy\n",
    "\n",
    "**Conclusion:** As we hypothesized, providing the model with two characters of context (trigram) instead of just one (bigram) allows it to better capture language patterns and make more accurate predictions. This demonstrates how increasing the context window in language models leads to improved performance, a principle that extends to modern large language models which use much larger context windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-world machine learning applications, we need to evaluate how well our models generalize to unseen data. To do this, we typically split our dataset into three parts:\n",
    "\n",
    "- **Training set (80%)**: Used to train the model parameters\n",
    "- **Validation/Dev set (10%)**: Used for hyperparameter tuning and model selection\n",
    "- **Test set (10%)**: Used only for final evaluation to estimate real-world performance\n",
    "\n",
    "In this exercise, we will:\n",
    "1. Randomly split our dataset following the 80:10:10 ratio\n",
    "2. Train our language models (bigram and trigram) **only** on the training set\n",
    "3. Evaluate performance on both validation and test sets\n",
    "4. Observe patterns in generalization performance\n",
    "\n",
    "**Key Questions:**\n",
    "- How do the models perform on unseen data compared to training data?\n",
    "- Which model generalizes better: bigram or trigram?\n",
    "- What does this tell us about the tradeoff between model complexity and generalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device).manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram model baseline\n",
    "\n",
    "Let's start by establishing a baseline using our familiar bigram model. This simpler model will help us understand:\n",
    "\n",
    "1. How well a basic model can generalize to new data\n",
    "2. Provide a comparison point for our more complex trigram model\n",
    "\n",
    "We'll reuse the same architecture as before, but now restrict training to just the training portion of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of all *bigrams*\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)  # [196113], [196113]\n",
    "num_x, num_y = xs.nelement(), ys.nelement()\n",
    "\n",
    "# Shuffle/Permute the dataset, keeping pairs in sync\n",
    "perm = torch.randperm(num_x)\n",
    "xs, ys = xs[perm], ys[perm]\n",
    "\n",
    "# Split 80:10:10 for train:valid:test\n",
    "xs_bi_train, xs_bi_valid, xs_bi_test = (\n",
    "    xs[: int(num_x * 0.8)],\n",
    "    xs[int(num_x * 0.8) : int(num_x * 0.9)],\n",
    "    xs[int(num_x * 0.9) :],\n",
    ")\n",
    "ys_bi_train, ys_bi_valid, ys_bi_test = (\n",
    "    ys[: int(num_x * 0.8)],\n",
    "    ys[int(num_x * 0.8) : int(num_x * 0.9)],\n",
    "    ys[int(num_x * 0.9) :],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27), device=device, generator=g, requires_grad=True)\n",
    "\n",
    "# Training cycles, using the entire dataset -> 200 Epochs\n",
    "for k in range(200):\n",
    "    # Forward pass\n",
    "    xenc = (\n",
    "        F.one_hot(xs_bi_train, num_classes=27).float().to(device)\n",
    "    )  # one-hot encode the names\n",
    "    logits = xenc @ W  # logits, different word for log-counts\n",
    "    counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(\n",
    "        1, keepdims=True\n",
    "    )  # Normal distribution probabilities (this is y_pred)\n",
    "    loss = (\n",
    "        -probs[torch.arange(len(probs)), ys_bi_train].log().mean()\n",
    "        + 0.01 * (W**2).mean()\n",
    "    )\n",
    "    print(f\"Loss @ iteration {k + 1}: {loss}\")\n",
    "    # Backward pass\n",
    "    W.grad = None  # Make sure all gradients are reset\n",
    "    loss.backward()  # Torch kept track of what this variable is, kinda cool\n",
    "    # Weight update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Bigram Training Performance\n",
    "\n",
    "The bigram model trained on our 80% training set reaches a final loss of $2.4826600551605225$.\n",
    "\n",
    "**Note**: This is slightly worse than when we trained on the full dataset (which is expected). The model has access to less information when trained on only 80% of the data.\n",
    "\n",
    "However, this approach offers a crucial advantage: we can now measure how well our model generalizes to unseen examples using our held-out validation and test sets. This will tell us whether our model is:\n",
    "\n",
    "- **Underfitting**: Performing poorly on both training and validation sets\n",
    "- **Overfitting**: Performing well on training but poorly on validation\n",
    "- **Generalizing well**: Performing similarly on both training and validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loss\n",
    "with torch.no_grad():\n",
    "    xenc = (\n",
    "        F.one_hot(xs_bi_valid, num_classes=27).float().to(device)\n",
    "    )  # one-hot encode the names\n",
    "    logits = xenc @ W  # logits, different word for log-counts\n",
    "    counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(\n",
    "        1, keepdims=True\n",
    "    )  # Normal distribution probabilities (this is y_pred)\n",
    "    loss = (\n",
    "        -probs[torch.arange(len(probs)), ys_bi_valid].log().mean()\n",
    "        + 0.01 * (W**2).mean()\n",
    "    )\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "\n",
    "# Test Loss\n",
    "with torch.no_grad():\n",
    "    xenc = (\n",
    "        F.one_hot(xs_bi_test, num_classes=27).float().to(device)\n",
    "    )  # one-hot encode the names\n",
    "    logits = xenc @ W  # logits, different word for log-counts\n",
    "    counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(\n",
    "        1, keepdims=True\n",
    "    )  # Normal distribution probabilities (this is y_pred)\n",
    "    loss = (\n",
    "        -probs[torch.arange(len(probs)), ys_bi_test].log().mean() + 0.01 * (W**2).mean()\n",
    "    )\n",
    "print(f\"Test Loss:\\t {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model Generalization Analysis\n",
    "\n",
    "**Key Observation**: The train, validation, and test losses are all very close to each other!\n",
    "\n",
    "This indicates that our bigram model generalizes well to unseen data. When a model performs similarly across all three data splits, it suggests that:\n",
    "\n",
    "1. The model has learned meaningful patterns rather than just memorizing the training data\n",
    "2. The statistical patterns of character sequences in names are relatively consistent across our dataset\n",
    "3. The bigram model's complexity level is appropriate for this particular task\n",
    "\n",
    "This good generalization makes sense intuitively - with only 27² = 729 possible bigrams, our model can easily encounter most meaningful character combinations during training, allowing it to handle similar patterns in the validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Bigram and Trigram Models\n",
    "\n",
    "Now that we've established our bigram baseline, let's implement and evaluate our trigram model on the same data splits. This comparison will help us understand the tradeoffs between:\n",
    "\n",
    "1. **Model complexity**: Trigrams capture more context but have more parameters\n",
    "2. **Generalization ability**: How well each model performs on unseen data\n",
    "3. **Sample efficiency**: How effectively each model learns from limited training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Trigram Generalization Performance\n",
    "\n",
    "**Hypothesis:** Will the trigram model generalize as well as the bigram model? Let's analyze this question before seeing the results.\n",
    "\n",
    "**Mathematical perspective**:\n",
    "- A trigram model must learn patterns for $27^3 = 19,683$ possible character combinations\n",
    "- A bigram model only needs to learn $27^2 = 729$ possible combinations\n",
    "- That's a 27x increase in the possible patterns to learn!\n",
    "\n",
    "**Model complexity implications**:\n",
    "- The trigram model's weight matrix `W` has dimensions $(27+27) \\times 27 = 54 \\times 27 = 1,458$ parameters\n",
    "- This larger parameter space creates a higher-dimensional optimization problem\n",
    "- While the model can theoretically capture more nuanced patterns in names, it needs more data to do so reliably\n",
    "\n",
    "**Expected generalization behavior**:\n",
    "- Due to this increased complexity, the trigram model will likely show some gap between training and validation/test performance\n",
    "- We expect both validation and test losses to be higher than the training loss\n",
    "- However, if the gap remains small, it would indicate that the additional complexity is justified by the improved modeling capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of all *trigrams*\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        xs.append([stoi[ch1], stoi[ch2]])\n",
    "        ys.append(stoi[ch3])\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)  # [196113, 2], [196113]\n",
    "num_x, num_y = xs.nelement() // 2, ys.nelement()\n",
    "\n",
    "# Shuffle/Permute the dataset, keeping (x,y) pairs in sync\n",
    "perm = torch.randperm(num_x)\n",
    "xs, ys = xs[perm, :], ys[perm]  # xs are shuffled along the zeroth dimension\n",
    "\n",
    "# Split 80:10:10 for train:valid:test\n",
    "xs_tri_train, xs_tri_valid, xs_tri_test = (\n",
    "    xs[: int(num_x * 0.8), :],\n",
    "    xs[int(num_x * 0.8) : int(num_x * 0.9), :],\n",
    "    xs[int(num_x * 0.9) :, :],\n",
    ")\n",
    "ys_tri_train, ys_tri_valid, ys_tri_test = (\n",
    "    ys[: int(num_x * 0.8)],\n",
    "    ys[int(num_x * 0.8) : int(num_x * 0.9)],\n",
    "    ys[int(num_x * 0.9) :],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(\n",
    "    (27 + 27, 27), device=device, generator=g, requires_grad=True\n",
    ")  # random column tensor of (27+27)x27 numbers (requires_grad=True for autograd)\n",
    "\n",
    "# Training cycles, using the entire dataset -> 200 Epochs, like the bigram model\n",
    "d_size = xs_tri_train.shape[0]\n",
    "for k in range(200):\n",
    "    # Forward pass\n",
    "    xenc = (\n",
    "        F.one_hot(xs_tri_train, num_classes=27).float().to(device)\n",
    "    )  # One-hot encoding, [196113, 2, 27]\n",
    "    xenc = xenc.view(d_size, -1)  # concatenate the one-hot vectors, [196113, 54]\n",
    "    logits = xenc @ W  # logits, different word for log-counts\n",
    "    counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(\n",
    "        1, keepdims=True\n",
    "    )  # Normal distribution probabilities (this is y_pred)\n",
    "    loss = (\n",
    "        -probs[torch.arange(d_size), ys_tri_train].log().mean() + 0.01 * (W**2).mean()\n",
    "    )\n",
    "    print(f\"Loss @ iteration {k + 1}: {loss}\")\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None  # Make sure all gradients are reset\n",
    "    loss.backward()  # Torch kept track of what this variable is, kinda cool\n",
    "\n",
    "    # Weight update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Training Results\n",
    "\n",
    "The training loss of our trigram model trained on 80% of the data reaches $2.2590503692626953$, which is virtually identical to the loss we achieved when training on the full dataset ($2.259373664855957$).\n",
    "\n",
    "**Interesting observation**: Despite having access to 20% less data, the trigram model achieves essentially the same training performance. This suggests:\n",
    "\n",
    "1. The 80% training set still contains most of the important trigram patterns present in the full dataset\n",
    "2. Our training process (200 epochs) is sufficient for the model to converge to a good solution\n",
    "3. The reduced dataset size doesn't significantly impact the model's ability to learn the core patterns\n",
    "\n",
    "Now let's evaluate this model on our validation and test sets to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loss\n",
    "d_size = xs_tri_valid.shape[0]\n",
    "with torch.no_grad():\n",
    "    xenc = (\n",
    "        F.one_hot(xs_tri_valid, num_classes=27).float().to(device)\n",
    "    )  # one-hot encode the names\n",
    "    xenc = xenc.view(d_size, -1)\n",
    "    logits = xenc @ W  # logits, different word for log-counts\n",
    "    counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(\n",
    "        1, keepdims=True\n",
    "    )  # Normal distribution probabilities (this is y_pred)\n",
    "    loss = (\n",
    "        -probs[torch.arange(d_size), ys_tri_valid].log().mean() + 0.01 * (W**2).mean()\n",
    "    )\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "\n",
    "# Test Loss\n",
    "d_size = xs_tri_test.shape[0]\n",
    "with torch.no_grad():\n",
    "    xenc = (\n",
    "        F.one_hot(xs_tri_test, num_classes=27).float().to(device)\n",
    "    )  # one-hot encode the names\n",
    "    xenc = xenc.view(d_size, -1)\n",
    "    logits = xenc @ W  # logits, different word for log-counts\n",
    "    counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(\n",
    "        1, keepdims=True\n",
    "    )  # Normal distribution probabilities (this is y_pred)\n",
    "    loss = -probs[torch.arange(d_size), ys_tri_test].log().mean() + 0.01 * (W**2).mean()\n",
    "print(f\"Test Loss:\\t {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Generalization Analysis\n",
    "\n",
    "**Results Summary**:\n",
    "- Both validation and test losses are slightly higher than the training loss, which aligns with our hypothesis\n",
    "- However, the difference is smaller than we might have expected given the significant increase in model complexity\n",
    "\n",
    "**Interpretation**:\n",
    "1. **Confirmation of complexity theory**: The increased complexity does lead to some generalization gap, as predicted\n",
    "2. **Surprisingly good generalization**: Despite having 27x more possible combinations to learn, the gap is quite small\n",
    "3. **Value of context**: The additional context captured by trigrams appears to provide genuinely useful information for prediction\n",
    "4. **Dataset characteristics**: The names dataset likely contains strong trigram patterns that are consistent across the data splits\n",
    "\n",
    "**Key insight**: The improved performance of the trigram model (lower overall loss compared to the bigram model) combined with its good generalization suggests that the additional complexity is justified for this task. The benefit of modeling longer contextual dependencies outweighs the increased risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of all *trigrams*\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        xs.append([stoi[ch1], stoi[ch2]])\n",
    "        ys.append(stoi[ch3])\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)  # [196113, 2], [196113]\n",
    "num_x, num_y = xs.nelement() // 2, ys.nelement()\n",
    "\n",
    "# Shuffle/Permute the dataset, keeping (x,y) pairs in sync\n",
    "perm = torch.randperm(num_x)\n",
    "xs, ys = xs[perm, :], ys[perm]  # xs are shuffled along the zeroth dimension\n",
    "\n",
    "# Split 80:10:10 for train:valid:test\n",
    "xs_tri_train, xs_tri_valid, xs_tri_test = (\n",
    "    xs[: int(num_x * 0.8), :],\n",
    "    xs[int(num_x * 0.8) : int(num_x * 0.9), :],\n",
    "    xs[int(num_x * 0.9) :, :],\n",
    ")\n",
    "ys_tri_train, ys_tri_valid, ys_tri_test = (\n",
    "    ys[: int(num_x * 0.8)],\n",
    "    ys[int(num_x * 0.8) : int(num_x * 0.9)],\n",
    "    ys[int(num_x * 0.9) :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Regularization Strength\n",
    "\n",
    "Now that we've established the basic performance of our trigram model, let's explore how regularization affects its generalization.\n",
    "\n",
    "**Regularization** adds a penalty to the loss function for large weight values, which can help prevent overfitting. The strength of this penalty is a hyperparameter we can tune.\n",
    "\n",
    "**Experimental setup**:\n",
    "- We'll sweep the regularization strength from $0.0$ (no regularization) to $1.0$ (strong regularization)\n",
    "- We'll use 25 evenly spaced values across this range for thorough coverage\n",
    "- For each strength value, we'll:\n",
    "  1. Train a complete trigram model from scratch\n",
    "  2. Evaluate it on all three data splits (train, validation, test)\n",
    "  3. Record the losses for later analysis\n",
    "\n",
    "**Selection process**:\n",
    "- We'll visualize all losses to understand the relationship between regularization and generalization\n",
    "- The optimal strength will be selected based on the validation loss (not the test loss)\n",
    "- This mimics real-world scenarios where the test set remains untouched until final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 0.0 to 1.0 in 25 steps\n",
    "strengths = torch.linspace(0.0, 1.0, 25, device=device)\n",
    "losst, lossv, lossf = [], [], []\n",
    "\n",
    "for strength in strengths:\n",
    "    W = torch.randn(\n",
    "        (27 + 27, 27), device=device, generator=g, requires_grad=True\n",
    "    )  # random column tensor of (27+27)x27 numbers (requires_grad=True for autograd)\n",
    "\n",
    "    # Training cycles, using the entire dataset -> 200 Epochs, like the bigram model\n",
    "    d_size = xs_tri_train.shape[0]\n",
    "    for k in range(200):\n",
    "        # Forward pass\n",
    "        xenc = (\n",
    "            F.one_hot(xs_tri_train, num_classes=27).float().to(device)\n",
    "        )  # One-hot encoding, [196113, 2, 27]\n",
    "        xenc = xenc.view(d_size, -1)  # concatenate the one-hot vectors, [196113, 54]\n",
    "        logits = xenc @ W  # logits, different word for log-counts\n",
    "        counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "        probs = counts / counts.sum(\n",
    "            1, keepdims=True\n",
    "        )  # Normal distribution probabilities (this is y_pred)\n",
    "        loss_t = (\n",
    "            -probs[torch.arange(d_size), ys_tri_train].log().mean()\n",
    "            + strength * (W**2).mean()\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        W.grad = None  # Make sure all gradients are reset\n",
    "        loss_t.backward()  # Torch kept track of what this variable is, kinda cool\n",
    "\n",
    "        # Weight update\n",
    "        W.data += -50 * W.grad\n",
    "\n",
    "    # Validation Loss\n",
    "    d_size = xs_tri_valid.shape[0]\n",
    "    with torch.no_grad():\n",
    "        xenc = (\n",
    "            F.one_hot(xs_tri_valid, num_classes=27).float().to(device)\n",
    "        )  # one-hot encode the names\n",
    "        xenc = xenc.view(d_size, -1)\n",
    "        logits = xenc @ W  # logits, different word for log-counts\n",
    "        counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "        probs = counts / counts.sum(\n",
    "            1, keepdims=True\n",
    "        )  # Normal distribution probabilities (this is y_pred)\n",
    "        loss_v = (\n",
    "            -probs[torch.arange(d_size), ys_tri_valid].log().mean()\n",
    "            + strength * (W**2).mean()\n",
    "        )\n",
    "\n",
    "    # Test Loss\n",
    "    d_size = xs_tri_test.shape[0]\n",
    "    with torch.no_grad():\n",
    "        xenc = (\n",
    "            F.one_hot(xs_tri_test, num_classes=27).float().to(device)\n",
    "        )  # one-hot encode the names\n",
    "        xenc = xenc.view(d_size, -1)\n",
    "        logits = xenc @ W  # logits, different word for log-counts\n",
    "        counts = (\n",
    "            logits.exp()\n",
    "        )  # 'fake counts' as we did for the N matrix of the Bigram model\n",
    "        probs = counts / counts.sum(\n",
    "            1, keepdims=True\n",
    "        )  # Normal distribution probabilities (this is y_pred)\n",
    "        loss_f = (\n",
    "            -probs[torch.arange(d_size), ys_tri_test].log().mean()\n",
    "            + strength * (W**2).mean()\n",
    "        )\n",
    "\n",
    "    # Note the losses for this strength\n",
    "    losst.append((strength, loss_t))\n",
    "    lossv.append((strength, loss_v))\n",
    "    lossf.append((strength, loss_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(\n",
    "    [y.item() for (_, y) in losst],\n",
    "    label=\"Train Loss\",\n",
    "    linestyle=\"-\",\n",
    "    marker=\"o\",\n",
    "    color=\"green\",\n",
    ")\n",
    "plt.plot(\n",
    "    [y.item() for (_, y) in lossv],\n",
    "    label=\"Validation Loss\",\n",
    "    linestyle=\"-\",\n",
    "    marker=\"o\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.plot(\n",
    "    [y.item() for (_, y) in lossf],\n",
    "    label=\"Test Loss\",\n",
    "    linestyle=\"-\",\n",
    "    marker=\"o\",\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Strength\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Report the best strength\n",
    "print(\n",
    "    f\"Best Strength: {min(lossv, key=lambda x: x[1])[0]} @ Train: {min(losst, key=lambda x: x[1])[1]}, Validation: {min(lossv, key=lambda x: x[1])[1]} & Test: {min(lossf, key=lambda x: x[1])[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Regularization Effects\n",
    "\n",
    "The results from our regularization experiments reveal several important insights:\n",
    "\n",
    "**Observed patterns**:\n",
    "- **Training loss**: Increases steadily with regularization strength in a logarithmic-like curve\n",
    "- **Validation loss**: Follows the training loss curve closely, also increasing with regularization strength\n",
    "- **Test loss**: Generally tracks with validation loss, with slightly sharper increases at lower regularization values\n",
    "- **Generalization gap**: The gap between training and test loss is largest at low regularization strengths\n",
    "\n",
    "**Understanding regularization mechanics**:\n",
    "- Regularization works by penalizing large weight values in the model\n",
    "- As regularization strength increases, the model is forced to use smaller weights\n",
    "- In our case, this constraint appears to limit the model's ability to capture important patterns\n",
    "- The result is higher loss across all data splits as regularization increases\n",
    "\n",
    "**Counterintuitive finding**:\n",
    "\n",
    "While regularization typically helps prevent overfitting, in this particular case:\n",
    "- The dataset is relatively small and structured\n",
    "- The trigram patterns are consistent and meaningful\n",
    "- The model complexity, while higher than bigrams, is still manageable for this data\n",
    "\n",
    "**Practical conclusion**:\n",
    "- For this specific language modeling task, minimal or no regularization produces the best results\n",
    "- This demonstrates how the ideal regularization strategy depends on the specific characteristics of the dataset and model\n",
    "- In larger, more complex models or noisier datasets, we might see more benefit from stronger regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(\n",
    "    (27 + 27, 27), device=device, generator=g, requires_grad=True\n",
    ")  # random column tensor of (27+27)x27 numbers (requires_grad=True for autograd)\n",
    "\n",
    "# Training cycles, using the entire dataset -> 200 Epochs, like the bigram model\n",
    "d_size = xs_tri_train.shape[0]\n",
    "for k in range(200):\n",
    "    # Forward pass\n",
    "    logits = (\n",
    "        W[xs_tri_train[:, 0]] + W[27 + xs_tri_train[:, 1]]\n",
    "    )  # logits, different word for log-counts\n",
    "    counts = logits.exp()  # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(\n",
    "        1, keepdims=True\n",
    "    )  # Normal distribution probabilities (this is y_pred)\n",
    "    loss = (\n",
    "        -probs[torch.arange(d_size), ys_tri_train].log().mean() + 0.01 * (W**2).mean()\n",
    "    )\n",
    "    print(f\"Loss @ iteration {k + 1}: {loss}\")\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None  # Make sure all gradients are reset\n",
    "    loss.backward()  # Torch kept track of what this variable is, kinda cool\n",
    "\n",
    "    # Weight update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Optimization: Direct Embedding Lookup\n",
    "\n",
    "**Understanding the Original Approach**\n",
    "\n",
    "The core idea of our original trigram model was:\n",
    "```python\n",
    "xenc = F.one_hot(xs_tri_train, num_classes=27).float().to(device) # One-hot encoding, [196113, 2, 27]\n",
    "xenc = xenc.view(d_size, -1) # concatenate the one-hot vectors, [196113, 54]\n",
    "logits = xenc @ W\n",
    "```\n",
    "\n",
    "This implementation:\n",
    "1. Converts each character to a one-hot vector (27 dimensions)\n",
    "2. Concatenates two one-hot vectors to form a 54-dimensional input vector\n",
    "3. Multiplies this with weight matrix `W` of shape $54 \\times 27$\n",
    "\n",
    "The weight matrix structure is significant:\n",
    "- First 27 rows correspond to weights for the first character\n",
    "- Next 27 rows correspond to weights for the second character\n",
    "- This allows the model to learn different weights for each character position\n",
    "\n",
    "**The Optimized Implementation**\n",
    "\n",
    "We can achieve the same mathematical result more efficiently:\n",
    "```python\n",
    "logits = W[xs_tri_train[:,0]] + W[27 + xs_tri_train[:,1]]\n",
    "```\n",
    "\n",
    "**How and why this works**:\n",
    "1. **Matrix multiplication with one-hot vectors is equivalent to lookup**: When you multiply a one-hot vector by a matrix, you're essentially selecting a specific row of that matrix\n",
    "2. **Direct indexing**: Instead of creating one-hot vectors and performing matrix multiplication, we directly index into the rows of `W` using character indices\n",
    "3. **Position encoding**: For the second character, we add 27 to the index to access the second half of the weight matrix\n",
    "4. **Addition combines influences**: The sum combines the influence of both characters on predicting the next character\n",
    "\n",
    "**Benefits**:\n",
    "- **Computational efficiency**: Eliminates the need to create and manipulate large one-hot vectors\n",
    "- **Memory efficiency**: Reduces memory usage during forward pass\n",
    "- **Mathematical equivalence**: Produces exactly the same results as the original approach\n",
    "\n",
    "This optimization illustrates an important principle in deep learning: understanding the mathematical equivalence between operations allows us to implement more efficient solutions without changing the underlying model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Change the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, the choice of loss function is crucial for effective training. So far, we've been implementing the negative log-likelihood loss manually. However, PyTorch provides optimized implementations of common loss functions.\n",
    "\n",
    "In this exercise, we will:\n",
    "1. Replace our manual negative log-likelihood implementation with PyTorch's built-in `F.cross_entropy`\n",
    "2. Compare the results to verify mathematical equivalence\n",
    "3. Analyze the advantages of using the built-in function\n",
    "\n",
    "**Key Questions:**\n",
    "- Does `F.cross_entropy` produce the same results as our manual implementation?\n",
    "- What are the technical advantages of using the built-in function?\n",
    "- Are there any performance benefits to this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch's Cross Entropy Loss\n",
    "\n",
    "PyTorch's `F.cross_entropy` ([docs](https://docs.pytorch.org/docs/main/generated/torch.nn.functional.cross_entropy.html)) function combines several operations in one efficient call:\n",
    "\n",
    "1. It applies a softmax function to convert logits to probabilities\n",
    "2. It then computes the negative log-likelihood of the correct class\n",
    "3. It averages the loss across all examples in the batch\n",
    "\n",
    "**Implementation Note:** The function expects:\n",
    "- First argument: raw logits (unnormalized scores) from the model\n",
    "- Second argument: target class indices (not one-hot encoded)\n",
    "\n",
    "Let's implement our trigram model using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(\n",
    "    (27 + 27, 27), device=device, generator=g, requires_grad=True\n",
    ")  # random column tensor of (27+27)x27 numbers (requires_grad=True for autograd)\n",
    "\n",
    "# Training cycles, using the entire dataset -> 200 Epochs, like the bigram model\n",
    "d_size = xs_tri_train.shape[0]\n",
    "for k in range(200):\n",
    "    # Forward pass\n",
    "    logits = (\n",
    "        W[xs_tri_train[:, 0]] + W[27 + xs_tri_train[:, 1]]\n",
    "    )  # logits, different word for log-counts\n",
    "    loss = F.cross_entropy(logits, ys_tri_train) + 0.01 * (W**2).mean()\n",
    "    print(f\"Loss @ iteration {k + 1}: {loss}\")\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None  # Make sure all gradients are reset\n",
    "    loss.backward()  # Torch kept track of what this variable is, kinda cool\n",
    "\n",
    "    # Weight update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Using `F.cross_entropy`\n",
    "\n",
    "The final loss achieved is equivalent to our manual implementation, but using `F.cross_entropy` offers several important advantages:\n",
    "\n",
    "**1. Code Simplicity and Readability**\n",
    "- Replaces multiple lines of complex operations with a single function call\n",
    "- Makes the code easier to read, maintain, and debug\n",
    "- Clarifies the intent of the computation\n",
    "\n",
    "**2. Numerical Stability**\n",
    "- Our manual implementation used these steps:\n",
    "```python\n",
    "counts = logits.exp() # Convert logits to unnormalized probabilities\n",
    "probs = counts / counts.sum(1, keepdims=True) # Normalize to get probabilities\n",
    "loss = -probs[torch.arange(d_size), ys_tri_train].log().mean() + 0.01 * (W**2).mean()\n",
    "```\n",
    "- This approach can cause numerical issues:\n",
    "  - The `exp()` operation can easily overflow for large logit values\n",
    "  - Taking the logarithm of very small probabilities can lead to underflow\n",
    "\n",
    "**3. Computational Efficiency**\n",
    "\n",
    "`F.cross_entropy` uses a mathematically equivalent but more stable approach:\n",
    "- Applies log-softmax operation that combines softmax and log in a single step\n",
    "- Computes operations in log-space to avoid overflow/underflow\n",
    "- Implements optimized CUDA kernels for faster computation on GPUs\n",
    "\n",
    "**4. Memory Efficiency**\n",
    "- Avoids creating intermediate tensors for probabilities\n",
    "- Reduces memory usage during the forward and backward passes\n",
    "\n",
    "These advantages make `F.cross_entropy` the preferred choice in production machine learning code, ensuring training is faster, more stable, and less prone to numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random column tensor of (27+27)x27 numbers (requires_grad=True for autograd)\n",
    "W = torch.randn(\n",
    "    (27, 27), device=device, generator=g, requires_grad=True\n",
    ")\n",
    "\n",
    "# Training cycles, using the entire dataset for 200 Epochs\n",
    "d_size = xs_tri_train.shape[0]\n",
    "for k in range(200):\n",
    "    # Forward pass\n",
    "    logits = (\n",
    "        W[xs_tri_train[:, 0]] + W[xs_tri_train[:, 1]]\n",
    "    )  # logits, different word for log-counts\n",
    "    loss = F.cross_entropy(logits, ys_tri_train) + 0.01 * (W**2).mean()\n",
    "    print(f\"Loss @ iteration {k + 1}: {loss}\")\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None  # Make sure all gradients are reset\n",
    "    loss.backward()  # Torch kept track of what this variable is, kinda cool\n",
    "\n",
    "    # Weight update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Model Architecture Importance\n",
    "\n",
    "The results from this experiment are clear, and they are **not good**:\n",
    "\n",
    "- The training loss is significantly higher than our original trigram model\n",
    "- Training behavior has become erratic and unstable\n",
    "- The model fails to learn effective representations\n",
    "\n",
    "**What went wrong?**\n",
    "\n",
    "In this final experiment, we modified the architecture of our model by changing the weight matrix dimensions from $(27+27) \\times 27$ to $27 \\times 27$. This seemingly small change had dramatic consequences:\n",
    "\n",
    "1. **Loss of position information**: By using the same weights for both character positions, we've eliminated the model's ability to learn position-specific representations\n",
    "\n",
    "2. **Reduced modeling capacity**: The number of parameters was reduced from 1,458 to 729, cutting the model's capacity in half\n",
    "\n",
    "3. **Parameter interference**: Updates to weights for one position now affect predictions for the other position, creating destructive interference\n",
    "\n",
    "**Key insight**: This experiment demonstrates that the proper architecture design is crucial for model performance. In language modeling, preserving position information through separate parameters for each position is essential for effective learning.\n",
    "\n",
    "This reinforces a fundamental principle in neural network design: the architecture should reflect the structure of the problem. For sequence modeling tasks like ours, position-specific parameters are vital for capturing the sequential nature of language."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
