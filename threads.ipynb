{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tutorial.tests.testsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism and concurrency in Python\n",
    "\n",
    "## Introduction\n",
    "There are many cases where we could execute multiple tasks in parallel or switch between tasks while we wait for some time consuming task to be completed.\n",
    "You can just think of examples from daily life to see why: \n",
    "\n",
    "- While your soup cooks on the stove, you start washing your dishes\n",
    "- You check the news while drinking your morning coffee\n",
    "- You work on your python course exercises while attending a video call\n",
    "\n",
    "Indeed, the language of computing is so engrained in many of us that today we refer to these sort of behaviors as *multitasking*, an expression borrowed from computer science.\n",
    "\n",
    "Because in computer science we like to be precise, let us define these terms better.\n",
    "\n",
    "## Parallelism Vs. concurrency\n",
    "\n",
    "### Parallelism\n",
    "When we have two or more tasks *running and progressing simultaneously*, we can talk about **parallelism**. \n",
    "Think for example of the situation of paying at the supermarket where there are multiple lines: more than one customer can pay their purchases at the same time\n",
    "\n",
    "### Concurrency\n",
    "When two or more tasks run in overlapping time periods (but **not necessarily simultaneously**) instead of sequentially, we say that their execution is **concurrent**.\n",
    "This is the typical human multitasking, where we work on multiple tasks in a time period, but we must switch between them to be able to perform them correctly.\n",
    "For example, we sit in a meeting, listen passively while working on our python program and stop working on our code to answer a question directed to us.\n",
    "\n",
    "\n",
    "\n",
    "The image below can help you understanding the difference between concurrent and parallel work.\n",
    "<figure>\n",
    "  <img\n",
    "  src=\"../../images/concurrency_vs_parallelism.jpg\"\n",
    "  height=\"400px\"\n",
    "  alt=\"The beautiful MDN logo.\">\n",
    "  <figcaption>A simple time diagram illustrating the difference between parallelism and concurrency (source: https://openclassrooms.com/en/courses/5684021-scale-up-your-code-with-java-concurrency/5684028-identify-the-advantages-of-concurrency-and-parallelism)</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Quiz: parallel or not\n",
    "For each of these real-life examples, determine if the tasks are executed in parallel or not\n",
    "\n",
    "- One cashier serves two lines of people in a store\n",
    "- A swimming pool offers multiple shower stalls \n",
    "- Multiple people take turns drinking from a cup\n",
    "\n",
    "\n",
    "## Parallelism in python: pre-emptive multitasking\n",
    "By default, in python tasks do not run in parallel. Consider this example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from time import sleep\n",
    "\n",
    "def task(name: str):\n",
    "    \"\"\"\n",
    "    This function defines a fictional task that takes one second\n",
    "    to complete and prints when it started and finished.\n",
    "    \"\"\"\n",
    "    print(f\"{name} started at {dt.now()}\")\n",
    "    sleep(1)\n",
    "    print(f\"{name} finished at {dt.now()}\")\n",
    "\n",
    "\n",
    "\n",
    "def two_tasks():\n",
    "    task(\"First task\")\n",
    "    task(\"Second task\")\n",
    "\n",
    "\n",
    "two_tasks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first task to be started (`First task`) finished before the second one could start. \n",
    "This is the sequential  computational model we are used to when we first learn programming. \n",
    "However, in python we can introduce **parallelism** by using the [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) module. \n",
    "\n",
    "Using this module, we can execute code in different operating system **processes**. \n",
    "A process is a representation of a task, with all the code, memory and resources (files, network connections, etc.) needed to run it. \n",
    "In most cases, processes are managed by the operating system, which takes care of scheduling what process should currently run and takes care that no process can run forever and regularly yields computing resources to other processes.\n",
    "This approach is called **preemptive multitasking** and is the standard way of running multiple processes in modern desktop and server operating systems. \n",
    "\n",
    "When working on a multi-core or multi-CPU system, it is possible to leverage multitasking to run your computations in parallel. \n",
    "We will see how in the following sections.\n",
    "\n",
    "### High-level interface: Process pools\n",
    "\n",
    "Let's rewrite our example from before using [`multiprocessing.Pool`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool) which executes jobs on a pool of shared processes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from time import sleep\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def task(name: str):\n",
    "    \"\"\"\n",
    "    This function defines a fictional task that takes one second\n",
    "    to complete and prints when it started and finished.\n",
    "    \"\"\"\n",
    "    print(f\"{name} started at {dt.now()}\")\n",
    "    sleep(1)\n",
    "    print(f\"{name} finished at {dt.now()}\")\n",
    "\n",
    "\n",
    "\n",
    "def two_tasks():\n",
    "    with Pool(3) as p:\n",
    "        p.map(task, [\"First task\", \"Second task\"])\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    two_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `Pool` as a [context manager](https://book.pythontips.com/en/latest/context_managers.html) and use the `map` method of the pool object to call the function `task` with a list of arguments. \n",
    "Internally, this will create and run a separate process for each value in the list.\n",
    "\n",
    "As you can see from the console output, the two task not only run simultaneously (**concurrently**) but also in parallel. \n",
    "This output highlights quite well one problem with concurrent computations: the order of completion is **non-deterministic**. \n",
    "We cannot know a priori which process will be started first and which process will complete first. \n",
    "If the order of the results is important, you need to make sure to send and return some sort of identifier with each job, so that you can reconstruct the right order.\n",
    "\n",
    "However, if we use `map`, it takes care of managing the order of tasks automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from time import sleep\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def increment(number: int) -> int:\n",
    "    \"\"\"\n",
    "    This function increments the number by 1.\n",
    "    \"\"\"\n",
    "    name = \"Process \" + str(number)\n",
    "    print(f\"{name} started at {dt.now()}\")\n",
    "    result = number + 1\n",
    "    print(f\"{name} finished at {dt.now()}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def two_tasks():\n",
    "    with Pool(3) as p:\n",
    "        res = p.map(increment, range(10))\n",
    "    print(res)\n",
    "\n",
    "\n",
    "\n",
    "two_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-level interface: Process, run, join and deadlocks\n",
    "\n",
    "In some situations, we want more control over the execution of multiple processes. \n",
    "In that case, you can directly create processes using the [`Process`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process) object. \n",
    "This offers several methods, primarily:\n",
    "- `run()`: by default, it runs the callable object with the argument passed at the process creation time. \n",
    "    <div class=\"alert alert-block alert-warning\">\n",
    "        <h4><b>Warning</b></h4> The <code>run</code> method is <b>blocking</b> and will just execute the function in the current python process, blocking it until the execution finishes.\n",
    "    </div>\n",
    "- `start()`: it will start the computation defined by `run()` in a separate process.\n",
    "- `join()`: this methods blocks the python interpreter process until the task defined by the owning `Process` finishes. \n",
    "     A process cannot join itself because this would cause a **deadlock**. This is a situation where there is a cycling dependency between some waiting resources. \n",
    "    In real life, imagine the situation of two friends waiting for each other to call before going out. `join` is useful when we want to make sure a given process finishes its job before continuing.\n",
    "\n",
    "let's see an example on how to use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def log(message: str):\n",
    "    \"\"\"\n",
    "    This function just prints a message.\n",
    "    \"\"\"\n",
    "    print(f\"{dt.now()}: {message}\")\n",
    "\n",
    "def friend(n: int, sleep_time: int = 1):\n",
    "    \"\"\"\n",
    "    This function prints a friendly message.\n",
    "    \"\"\"\n",
    "    log(f\"Hello from process {n}\")\n",
    "    sleep(sleep_time)\n",
    "    log(f\"After sleeping, process {n} is done\")\n",
    "\n",
    "def waiting_friend(n: int, wait_for_friend: bool = True, sleep_time: int = 1):\n",
    "    \"\"\"\n",
    "    This function waits for n friend processes to finish.\n",
    "    \"\"\"\n",
    "    f = [Process(target=friend, args=(i, sleep_time)) for i in range(n)]\n",
    "    for p in f:\n",
    "        p.start()\n",
    "        if wait_for_friend:\n",
    "            p.join()\n",
    "    log(\"Finished\")\n",
    "\n",
    "\n",
    "\n",
    "#Start without waiting for friend\n",
    "waiting_friend(3, True, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the effect of `join` in the output of `waiting_friend`. \n",
    "If we do not `join` on the child processes, they can finish in any order; we don't have any guarantee that they will finish at all. \n",
    "Indeed, you can see that `waiting_friends` prints \"finished\" before any of the child processes wakes up from sleep.  \n",
    "\n",
    "On the other hand, if we join on them in the for loop by setting the second argument of `waiting_friend` to `True`, the function `waiting_friend` will wait for each friend (1, 2, 3) to finish before starting the next process.\n",
    "\n",
    "### Higher level interface: concurrent.futures\n",
    "\n",
    "Because `join` **blocks** the main process until the child process finished, this is rarely the solution we need for scientific computing, where we want to split a large unit of work into smaller blocks and have multiple processes handle each of the blocks independently.\n",
    "For this reason, in most cases we advise starting your parallel processing adventure using higher-level solutions that take care of this low-level synchronization for you. \n",
    "A first good starting point is the [`concurrent.futures`](https://docs.python.org/3/library/concurrent.futures.html) module of the python standard library. \n",
    "This module wraps the methods of `multiprocessing` (and `multithreading`) modules and offers more convenient ways to launch parallel computations.\n",
    "\n",
    "The way it works is roughly as follows:\n",
    "\n",
    "- It creates a *pool* of `n` worker processes\n",
    "- It sends the functions (and the data) to execute to the the processes in batches of size `n`\n",
    "- It waits (joins) to the processes until they finish.\n",
    "- It repeats the steps above until it exhausts the data to process\n",
    "\n",
    "Let's see how this works in practice with an artificial example: we call a function `work` that receives a number `i`, then waits 0.1 seconds and then return the number `i`. \n",
    "We want to call this function `n` times and compute the sum of the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from itertools import count, groupby, islice\n",
    "from time import sleep\n",
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "def work(n: int) -> int:\n",
    "    pid = os.getpid()\n",
    "    print(f\"{pid} Working on {n}\\n\")\n",
    "    sleep(0.01)\n",
    "    return n\n",
    "\n",
    "\n",
    "def parallel_work(executor: ProcessPoolExecutor, n: int, batch_size=5) -> int:\n",
    "    res = executor.map(work, range(n), chunksize=batch_size)\n",
    "    return sum(res)\n",
    "    \n",
    "\n",
    "def sequential_work(n: int) -> int:\n",
    "    \"\"\"\n",
    "    This function returns the sum of the squares of the first n numbers.\n",
    "    \"\"\"\n",
    "    return sum([work(i) for i in range(n)])\n",
    "\n",
    "n = 10\n",
    "with ProcessPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "    res = parallel_work(executor, n)\n",
    "    res1 = sequential_work(n)\n",
    "print(f\"The parallel sum is {res}, the regular is {res1}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! We can see that the sum computed in parallel using multiple processes equals the sum computed *locally* in one python process. Now it is interesting to try and see how much speedup we gain from this trick. To do so, we use the `timeit` module part of the python standard library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def time_for(fun: callable, args: list[any]):\n",
    "    return [{arg: timeit.repeat(lambda : fun(arg), number=1, repeat=3)[1]} for arg in args]\n",
    "\n",
    "\n",
    "sizes = range(1, 100, 1)\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "    res = time_for(lambda i : parallel_work(executor, i, ), sizes)\n",
    "    res1 = time_for(sequential_work, sizes)\n",
    "    \n",
    "print(res)\n",
    "print(res1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Axes\n",
    "\n",
    "def plot_times(ax: Axes, times: list[dict[str, float]], title: str):\n",
    "    return ax.scatter(sizes, [list(t.values())[0] for t in times], label=title)\n",
    "\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(1, 1)\n",
    "plot_times(ax, res, \"Parallel\")\n",
    "plot_times(ax, res1, \"Sequential\")\n",
    "h1 = ax.set_xlabel(\"Size\")\n",
    "h2 = ax.set_ylabel(\"Time\")\n",
    "f.legend()\n",
    "f.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "  <img\n",
    "  src=\"../../images/process_performance.png\"\n",
    "  height=\"400px\"\n",
    "  alt=\"The beautiful MDN logo.\">\n",
    "  <figcaption>Comparison of the runtime (in seconds) of the <i>parallel_work</i> and <i>sequential_work</i> functions as a function of the input size.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the graph, there is a clear difference in execution time between `sequential_work` and `parallel_work` as the number `n` increases. \n",
    "This is due to the fact that if we execute the function `work` sequentially for `n` inputs, we need to wait **at least** `n` times the duration of execution of `work`. \n",
    "On the other hand, if we process in parallel, we can cut down the time by maximally `n_proc` where `n_proc` is the number of CPUs our system offers. \n",
    "In reality, the speed up will be a bit smaller because of the overhead of starting different processes and the synchronization effort.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <h4><b>Warning</b></h4> In reality, multiprocessing/parallelism in python should be the last resort to improve the performance of your code. \n",
    "    You first should try improving your code by using the appropriate algorithms or by adopting numerical libraries like numpy that offer vectorized operations. \n",
    "    If this fails, you could try just-in-time compilers like numba, which only require you to add a decorator to existing functions. Only if all these steps yield no improvement should you attempt to use multiprocessing explicitly. \n",
    "    If not used carefully, it can even harm the performance of your code.\n",
    "</div>\n",
    "\n",
    "\n",
    "### Inter-process communication and data dependencies\n",
    "\n",
    "The pattern of using `ProcessPoolExecutor` works well for a variety of tasks where we can split the  problem into **independent** unit of works that can be performed at the same time; sometimes these problems are called *embarrassingly parallel*.  \n",
    "However, many problems in scientific computing contain **data dependencies** where computations depend on the result of other computations. In this case, we cannot simply split our input data across processes as we did before. In these situations, we have a few possible solutions:\n",
    "\n",
    "1. Split the problem across a dimension where there are no dependencies. \n",
    "1. Add communication between processes, so that individual processes can access the result computed by other processes.\n",
    "\n",
    "In the rest of this section, we will address the second solution, as python offers methods to communicate between processes. The first solution cannot be easily addressed in this course because it requires domain knowledge and  we cannot provide a simple recipe that will work in all cases.\n",
    "\n",
    "We now consider an example of a problem that can be parallelized but where there are dependencies between the processes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "def heat_equation(current, left, right, dt, dx, k):\n",
    "    alpha = k * dt / dx**2\n",
    "    return current + alpha * (left - 2*current + right)\n",
    "\n",
    "def integrate(block, left_temp, right_temp, dt, dx, alpha):\n",
    "    new_block = np.copy(block)\n",
    "    for i in range(1, len(block)-1):\n",
    "        new_block[i] = heat_equation(block[i], block[i-1], block[i+1], dt, dx, alpha)\n",
    "    new_block[0] = heat_equation(block[0], left_temp, block[1], dt, dx, alpha)\n",
    "    new_block[-1] = heat_equation(block[-1], block[-2], right_temp, dt, dx, alpha)\n",
    "    return new_block\n",
    "\n",
    "def process_block(block_id, block, left_temp, right_temp, dt, dx, alpha, queue):\n",
    "    new_block = integrate(block, left_temp, right_temp, dt, dx, alpha)\n",
    "    queue.put((block_id, new_block))\n",
    "\n",
    "rod = np.zeros(100)\n",
    "rod[0] = 1  # initial heat source\n",
    "dt = 0.0001\n",
    "dx = 0.1\n",
    "k = 0.001\n",
    "block_size = 25\n",
    "queue = multiprocessing.Queue()\n",
    "n_steps = 100\n",
    "f, ax = plt.subplots(1, 1)\n",
    "\n",
    "for _ in range(n_steps):  # 100 time steps\n",
    "    processes = []\n",
    "    for i in range(0, len(rod), block_size):\n",
    "        block = rod[i:i+block_size]\n",
    "        left_temp = rod[i-1] if i > 0 else 0\n",
    "        right_temp = rod[i+block_size] if i+block_size < len(rod) else 0\n",
    "        process = multiprocessing.Process(target=process_block, args=(i//block_size, block, left_temp, right_temp, dt, dx, k, queue))\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "    while not queue.empty():\n",
    "        block_id, new_block = queue.get()\n",
    "        rod[block_id*block_size:(block_id+1)*block_size] = new_block\n",
    "        ax.plot(rod)\n",
    "        f.show()\n",
    "    print(_, rod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "from numpy.typing import ArrayLike\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Axes\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "def init_queues(n_blocks: int) -> list[multiprocessing.Queue]:\n",
    "    queues = [multiprocessing.Queue() for _ in range(n_blocks+1)]\n",
    "    queues[0].put(0)\n",
    "    queues[-1].put(0)\n",
    "    return queues\n",
    "\n",
    "def get_queues(queues: list[multiprocessing.Queue], block_id: int) -> (Optional[multiprocessing.Queue], Optional[multiprocessing.Queue],):\n",
    "    left_queue = queues[block_id] if block_id > 0 else None\n",
    "    right_queue = queues[block_id+1] if block_id < len(queues) - 1 else None\n",
    "    return left_queue, right_queue\n",
    "\n",
    "def get_block_index(block_id: int, block_size: int) -> (int, int):\n",
    "    start = block_id * block_size\n",
    "    end = start + block_size\n",
    "    return start, end\n",
    "\n",
    "def second_derivative(temperatures: ArrayLike, dx: float) -> ArrayLike:\n",
    "    return (temperatures[2:] - 2*temperatures[1:-1] + temperatures[:-2])/dx**2\n",
    "\n",
    "def update_temperature(block_id: int, temperatures: ArrayLike, dx: float, dt: float, k, left_queue: Optional[multiprocessing.Queue] , right_queue: Optional[multiprocessing.Queue]):\n",
    "    start, end = get_block_index(block_id, block_size)\n",
    "    old_temperatures = temperatures[start:end].copy()\n",
    "    new_temperatures = np.zeros_like(old_temperatures)\n",
    "    alpha = k * dt / dx**2\n",
    "    new_temperatures[1:-1] = old_temperatures[1:-1] +  alpha * second_derivative(old_temperatures, dx)\n",
    "    if left_queue:\n",
    "        left_queue.put(new_temperatures[0])\n",
    "    if right_queue:\n",
    "        right_queue.put(new_temperatures[-1])\n",
    "    return new_temperatures\n",
    "dx = 0.01\n",
    "dt = 0.01\n",
    "alpha = 0.01\n",
    "block_size = 100\n",
    "n_steps = 1000\n",
    "n_blocks = 10\n",
    "\n",
    "temperatures = np.array([0]*block_size*n_blocks)\n",
    "temperatures[block_size*n_blocks//2] = 100  # initial heat source\n",
    "\n",
    "\n",
    "\n",
    "def make_animate(ax: Axes, temperatures, dx, dt, alpha, block_size, n_blocks):\n",
    "    queues = init_queues(n_blocks)\n",
    "    processes = []\n",
    "    line, = ax.plot(temperatures)\n",
    "    for block_id in range(n_blocks):\n",
    "        left_queue, right_queue = get_queues(queues, block_id)\n",
    "        process = multiprocessing.Process(target=update_temperature, args=(block_id, temperatures, dx, dt, alpha, left_queue, right_queue))\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "\n",
    "    def animate(i):\n",
    "        for block_id in range(n_blocks):\n",
    "            new_temperatures = queues[block_id].get()  # get new temperatures from the queue\n",
    "            start, end = get_block_index(block_id, block_size)\n",
    "            temperatures[start:end] = new_temperatures\n",
    "            print(i)\n",
    "\n",
    "        line.set_ydata(temperatures)\n",
    "        return line,\n",
    "\n",
    "    return animate\n",
    "fig, ax = plt.subplots()\n",
    "af = make_animate(ax, temperatures, dx, dt, alpha, block_size, n_blocks)\n",
    "ani = FuncAnimation(fig, af , frames=100, interval=2, blit=True)\n",
    "#ani.save('heat_equation.mpg', writer='ffmpeg', fps=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads, GIL and the illusion of concurrency\n",
    "\n",
    "Thus far, we only discussed one approach to concurrency (and parallelism) in python:  the use of operating system **processes** leveraging the ability of the OS to schedule and coordinate multiple processes across multiple CPUs. \n",
    "In reality, there's a second, very similar approach commonly called **threading**. \n",
    "Like a **process**, a **thread** is a representation of a task including all needed resources. \n",
    "In fact, a process usually consist of multiple threads all sharing a common memory. \n",
    "Because of shared memory, a thread is usually lighter in its resource usage than a process, moreover the shared memory means that multiple threads can communicate using shared variables, although this style of concurrency comes with severe performance and safety pitfalls and it is being increasingly discouraged. \n",
    "Therefore, in most programming languages we try to achieve concurrency by starting multiple threads in a process.  \n",
    "However, due to the way the python interpreter is written (the infamous [global interpreter lock](https://wiki.python.org/moin/GlobalInterpreterLock)), we cannot get any performance benefits from running python code across multiple threads: only one thread can perform CPU operations at a time. \n",
    "There are however some cases where we could benefit from this style: when we have multiple threads sitting and waiting from input from the network or from some other process. \n",
    "This is a case commonly encountered in user interface programming, where we don't want the main user interface to block waiting for the program to fetch data from the network.\n",
    "In that situation, we can run the GUI in a main thread and have a second thread being responsible for network interaction. \n",
    "Here we won't have any speedup but we will give the user the **illusion** of concurrency because the GUI won't freeze while the network threads goes and gets the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous programming and couroutines: cooperative multitasking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Counting words in a fileüå∂Ô∏èüå∂Ô∏è\n",
    "\n",
    "Write a **parallel** function `count_lines` that counts the number of words in the large file `input_file`\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h4><b>Hints</b></h4>\n",
    "    <ul>\n",
    "        <li>\n",
    "            You can open the file <b>read-only</b> multiple times. \n",
    "        </li>\n",
    "        <li>\n",
    "            Using <code>seek</code> you can specify a line offset from the start of the file. Using <code>read(size)</code> you can read <code>size</code> charcters only. \n",
    "        </li>\n",
    "        <li>\n",
    "            Write your function in the cell below inside of the <code>solution_exercise1</code> function. The function receives a <code>Path</code> object <code>input_file</code> as an input and should return a single <code>int</code>.\n",
    "        </li>\n",
    "    </ul>\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_exercise1():\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "from pathlib import Path\n",
    "def solution_exercise1(input_file: Path) -> int:\n",
    "    return 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
