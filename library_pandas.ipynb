{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03ea0d4-7724-4e00-b447-0b3f84c88396",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8972a3-f9d8-48fe-a7d3-8c5539afa747",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "  - [Pandas](#Pandas)\n",
    "    - [Introduction](#Introduction)\n",
    "      - [What is Pandas?](#What-is-Pandas?)\n",
    "      - [Why We Need Pandas with Python](#Why-We-Need-Pandas-with-Python)\n",
    "      - [When and Why It's Used](#When-and-Why-It's-Used)\n",
    "      - [Pandas Performance](#Pandas-Performance)\n",
    "      - [Alternatives and When They Are Better](#Alternatives-and-When-They-Are-Better)\n",
    "    - [Working with Pandas `DataFrame`](#Working-with-Pandas-DataFrame)\n",
    "      - [Pandas Data Structures](#Pandas-Data-Structures)\n",
    "        - [Working with NumPy Arrays](#Working-with-NumPy-Arrays)\n",
    "        - [`Series`](#Series)\n",
    "        - [`Index`](#Index)\n",
    "        - [`DataFrame`](#DataFrame)\n",
    "      - [Creating `DataFrame` objects](#Creating-DataFrame-objects)\n",
    "        - [Creating a `Series` object](#Creating-a-Series-object)\n",
    "        - [Creating a `DataFrame` object from a `Series` object](#Creating-a-DataFrame-object-from-a-Series-object)\n",
    "        - [Creating a `DataFrame` from Python Data Structures](#Creating-a-DataFrame-from-Python-Data-Structures)\n",
    "          - [From a dictionary of list-like structures](#From-a-dictionary-of-list-like-structures)\n",
    "          - [From a list of dictionaries](#From-a-list-of-dictionaries)\n",
    "          - [From a list of tuples](#From-a-list-of-tuples)\n",
    "          - [From a NumPy array](#From-a-NumPy-array)\n",
    "      - [Creating a `DataFrame` object from the contents of a CSV File](#Creating-a-DataFrame-object-from-the-contents-of-a-CSV-File)\n",
    "        - [Finding information on the file before reading it in](#Finding-information-on-the-file-before-reading-it-in)\n",
    "          - [Examining a few rows](#Examining-a-few-rows)\n",
    "          - [Column count](#Column-count)\n",
    "        - [Reading in the file](#Reading-in-the-file)\n",
    "        - [Writing a `DataFrame` Object to a CSV File](#Writing-a-DataFrame-Object-to-a-CSV-File)\n",
    "      - [Writing a `DataFrame` Object to a Database](#Writing-a-DataFrame-Object-to-a-Database)\n",
    "      - [Creating a `DataFrame` Object by Querying a Database](#Creating-a-DataFrame-Object-by-Querying-a-Database)\n",
    "    - [Inspecting data](#Inspecting-data)\n",
    "    - [Aggregating data](#Aggregating-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a054a4c-a2d0-4bd6-9bd7-b32c40364ae4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a123b2d-64cd-43f7-9255-567ca8d817d1",
   "metadata": {},
   "source": [
    "### What is Pandas?\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/docs/index.html) is an open-source data manipulation and analysis library for Python.\n",
    "It provides powerful data structures and functions designed to make working with structured data intuitive and efficient. At the heart of Pandas are two primary data structures:\n",
    "\n",
    "- **DataFrame**: A two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (rows and columns). It's similar to a spreadsheet or SQL table and is generally the most commonly used Pandas object.\n",
    "- **Series**: A one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.).\n",
    "\n",
    "Pandas integrates well with various other Python libraries, such as Matplotlib for plotting and NumPy for numerical computations, making it a central library in the Python data science stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64075e7a-3ca7-41dc-b105-bf2b50c13541",
   "metadata": {},
   "source": [
    "### Why We Need Pandas with Python\n",
    "\n",
    "Python, while a powerful programming language, isn't designed specifically for data analysis.\n",
    "It lacks built-in, high-level data structures and tools that are intuitive and efficient for these tasks.\n",
    "Here's where Pandas comes in:\n",
    "\n",
    "- **Data Cleaning and Preparation**: Data scientists spend a significant amount of time cleaning and preparing data. Pandas simplifies these tasks with built-in functions for filtering, selecting, and manipulating data.\n",
    "- **Data Analysis**: With Pandas, analyzing and exploring data is more straightforward. It provides functions for aggregating, summarizing, and transforming data, making it easier to derive insights.\n",
    "- **Data Visualization**: Though Pandas is not a data visualization library, it seamlessly interfaces with Matplotlib for plotting and visualizing data, allowing quick and informative visual analysis.\n",
    "- **Handling Diverse Data Types**: Pandas efficiently handles a variety of data formats, including CSV, Excel files, SQL databases, and HDF5 format, making it a versatile tool for diverse data analysis needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27059d1-124b-4247-ae63-55601f7e4cb6",
   "metadata": {},
   "source": [
    "### When and Why It's Used\n",
    "\n",
    "Pandas is widely used in a variety of fields for data analysis and manipulation tasks.\n",
    "Some common use cases include:\n",
    "\n",
    "- **Data Cleaning**: Transforming raw data into a form that is suitable for analysis, such as filling missing values, removing duplicates, and converting data types.\n",
    "- **Data Exploration and Analysis**: Quick examination of data for patterns, irregularities, and insights. This includes operations like sorting, filtering, and grouping data.\n",
    "- **Data Visualization**: Creating plots and graphs to understand trends and patterns in data.\n",
    "- **Machine Learning**: Preprocessing and cleaning datasets before feeding them into machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2422a58-51f0-46e7-814c-5c7966b0d578",
   "metadata": {},
   "source": [
    "### Pandas Performance\n",
    "\n",
    "Pandas is highly efficient for most data manipulation and analysis tasks, especially with small to moderately sized datasets.\n",
    "It's optimized for performance in many scenarios, with critical code paths written in Cython or C.\n",
    "However, when working with very large datasets (with about tens of millions of rows or more), Pandas can face performance issues due to:\n",
    "\n",
    "- **Memory Usage**: Pandas typically requires significantly more memory than the size of the data, making it less efficient for very large datasets.\n",
    "- **Speed**: For extremely large datasets, some operations in Pandas can be slow, as it's not fully optimized for all use cases, especially those involving large-scale, distributed computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e4f81-6f20-44e4-9e13-4fb514522782",
   "metadata": {},
   "source": [
    "### Alternatives and When They Are Better\n",
    "\n",
    "One of the notable alternatives to Pandas is [**Polars**](https://pola.rs/).\n",
    "Polars is a DataFrame library that is designed to handle larger datasets more efficiently than Pandas.\n",
    "Here's why and when Polars can be a better choice:\n",
    "\n",
    "- **Performance**: Polars is designed to be faster and more memory-efficient than Pandas, particularly with large datasets. It leverages modern hardware capabilities, like multi-threading, to speed up data processing.\n",
    "- **Lazy Evaluation**: Polars supports lazy evaluation, where computations are queued and executed only when necessary. This approach can lead to performance improvements, especially in complex data pipelines.\n",
    "- **Ease of Scaling**: For large-scale data processing, Polars can be a better fit. It's more adept at handling the kinds of big data tasks that are increasingly common in industry settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7aebe-e8c7-48e3-a801-acb76f6be285",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15fe23-17cc-479f-abd0-adeaf617b2e2",
   "metadata": {},
   "source": [
    "## Working with Pandas `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3c0c3-33c0-4062-a3d6-60e401d7486a",
   "metadata": {},
   "source": [
    "### Pandas Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5e05e-d681-4e01-9f79-b9dc1551060c",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we will discuss the `Series`, `Index`, and `DataFrame` classes. To do so, we will read in a snippet of the CSV file we will work with later. Don't worry about that part yet, though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018ca94-bd00-465e-a423-77668ccf303b",
   "metadata": {},
   "source": [
    "#### Working with NumPy Arrays\n",
    "Let's read in a short CSV file (using `numpy`) for some sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6609f1-a037-41ac-9d40-9d84948b3144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt(\n",
    "    'data/01/example_data.csv', delimiter=';', \n",
    "    names=True, dtype=None, encoding='UTF'\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52faac6-686a-45ed-adeb-13ed264f3746",
   "metadata": {},
   "source": [
    "We can find the dimensions with the `shape` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d423ff-7558-4a0b-b02a-c1ab46fa5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64774481-9c6e-4605-a237-660fdeabfede",
   "metadata": {},
   "source": [
    "We can find the data types with the `dtype` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e95237-eb16-403b-9ea6-43e23f100f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dfa6b-8822-4c32-af12-98674d6e5941",
   "metadata": {},
   "source": [
    "Each element in the array corresponds to a row from the CSV file. Unlike lists that can hold multiple data types, NumPy arrays are limited to one, enabling quick, vectorized actions. The data import resulted in an array of `numpy.void` objects, designed to handle various types. This occurs as each row contains diverse data types: four strings, one float, and one integer. Consequently, we miss out on the performance benefits NumPy offers for arrays with uniform data types.\n",
    "\n",
    "Consider finding the highest magnitude. We can employ a [list comprehension](https://www.python.org/dev/peps/pep-0202/) to extract the third index from each row, which is in the form of a `numpy.void` object. By doing this, we create a list that allows us to determine the maximum value using the `max()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37861f-d2dd-4bef-993e-19a2b054fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "max([row[3] for row in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765125b-dbfd-45ed-8a2c-50f6a49adb2b",
   "metadata": {},
   "source": [
    "If we, instead, create a NumPy array for each column, this operation is much easier (and more efficient) to perform. We can use a **[dictionary comprehension](https://www.python.org/dev/peps/pep-0274/)** to make a dictionary where the keys are the column names and the values are NumPy arrays of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1ff1f-21f5-4703-baf1-e27b7e5f0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_dict = {\n",
    "    col: np.array([row[i] for row in data])\n",
    "    for i, col in enumerate(data.dtype.names)\n",
    "}\n",
    "array_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d03eb-3a04-4055-aa57-01b112669ce0",
   "metadata": {},
   "source": [
    "Grabbing the maximum magnitude is now simply a matter of selecting the `mag` key and calling the `max()` method. This is nearly twice as fast as the list comprehension implementation when dealing with just 5 entries, imagine how much worse the first attempt will perform on large data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205f482-d065-4ad6-a456-c0855690de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "array_dict['mag'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e68fd-65d6-48d8-a1b6-0d3b3c8f8151",
   "metadata": {},
   "source": [
    "However, this representation has other issues. Say we wanted to grab all the information for the earthquake with the maximum magnitude, how would we go about that? We would need to find the index of the maximum and then for each of the keys in the dictionary grab that index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ff128-69ab-40ec-9e9c-866a5100f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([\n",
    "    value[array_dict['mag'].argmax()] \n",
    "    for key, value in array_dict.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67e3a9-2fcf-4bbc-8cab-49379b663e43",
   "metadata": {},
   "source": [
    "We now have a NumPy array consisting solely of strings, converting our numerical values into this format and reverting to the earlier setup. Additionally, if we aim to sort the data by magnitude in ascending order, the initial format requires sorting the rows based on the third index. In the second format, we need to establish the sorting order based on the `mag` column and then rearrange all other arrays accordingly. Handling multiple NumPy arrays with different data types simultaneously can be challenging. This is where `pandas` comes into play, enhancing the ease of working with NumPy arrays. Let's begin delving into `pandas` by understanding its data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f02e36-1531-46af-953a-179ccf8b01b1",
   "metadata": {},
   "source": [
    "#### `Series`\n",
    "The `Series` class provides a data structure for arrays of a single type with some additional functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b4b52-b3d9-421e-88e1-4d19935ea384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "place = pd.Series(array_dict['place'], name='place')\n",
    "place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b639a-4102-4b01-9e2d-6dc8feacf29f",
   "metadata": {},
   "source": [
    "Here are some commonly used attributes with `Series` objects:\n",
    "\n",
    "|Attribute | Returns |\n",
    "| --- | --- |\n",
    "| `name` | The name of the `Series` object |\n",
    "| `dtype` | The data type of the `Series` object |\n",
    "| `shape` | Dimensions of the `Series` object in a tuple of the form `(number of rows,)` |\n",
    "| `index` | The `Index` object that is part of the `Series` object |\n",
    "| `values` | The data in the `Series` object |\n",
    "\n",
    "For the most part, `pandas` objects use NumPy arrays for their internal data representations. However, for some data types, `pandas` builds upon NumPy to create its own [arrays](https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html). For this reason, depending on the data type, `values` can either be a `pandas.array` or `numpy.array` object. Therefore, if we need to ensure we get a specific type back, then it is recommended to use the `array` attribute or `to_numpy()` method, respectively, instead of `values`.\n",
    "\n",
    "Now let's see some examples using these attributes.\n",
    "\n",
    "**Getting the name of the series**\n",
    "\n",
    "The NumPy array held the name of the data in the `dtype` attribute; here, we can access it directly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d061fb7-8efe-44cf-9752-9e710a7c2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "place.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9474613-7dde-4bc9-a12f-bda893948085",
   "metadata": {},
   "source": [
    "**Getting the data type**\n",
    "\n",
    "A `Series` object holds a single data type.\n",
    "Here it is `'O'` for object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38c3f3-e182-4349-b6e8-d8e5e450a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "place.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9d9e6-3ea2-47e4-ba1c-5e3ca840a084",
   "metadata": {},
   "source": [
    "**Getting the dimensions of the series**\n",
    "\n",
    "Just as with NumPy, we can use `shape` to get the dimensions as `(rows, columns)`.\n",
    "`Series` objects are a single column, so they only have values for the rows dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999677ee-f317-4771-9348-65ec2fd2411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "place.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566722c-7fd2-4256-82e4-78d09e3d42e2",
   "metadata": {},
   "source": [
    "**Isolating the values from the series**\n",
    "\n",
    "This `Series` object is storing its values as a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfbf79-086c-40c1-96a7-532e9620d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "place.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9a8a6-6224-44d9-99fa-1a1d726fdecb",
   "metadata": {},
   "source": [
    "#### `Index`\n",
    "The addition of the `Index` class makes the `Series` class more powerful than a NumPy array. We can get the index from the `index` attribute of a `Series` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfab39-539a-482c-8307-a922a58cac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_index = place.index\n",
    "place_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bdf54f-58c0-4d8e-ab8a-5d8a5b138c3f",
   "metadata": {},
   "source": [
    "As with `Series` objects, we can access the underlying data via the `values` attribute. Note that this `Index` object is also built on top of a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f400e54-1f0a-4ecb-9e83-d1deb64a44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a27615-81b6-4afd-afde-6df0f991e64c",
   "metadata": {},
   "source": [
    "Here are some commonly used attributes with `Index` objects:\n",
    "\n",
    "|Attribute | Returns |\n",
    "| --- | --- |\n",
    "| `name` | The name of the `Index` object |\n",
    "| `dtype` | The data type of the `Index` object |\n",
    "| `shape` | Dimensions of the `Index` object |\n",
    "| `values` | The data in the `Index` object |\n",
    "| `is_unique` | Check if the `Index` object has all unique values |\n",
    "\n",
    "We can check the type of the underlying data, just like with a `Series` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9e50f-b199-41da-b6b3-179dec13836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_index.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5126c-7297-4502-b99f-3b320db48276",
   "metadata": {},
   "source": [
    "Same for the dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca8016-1534-428b-a4b3-e98cb90a9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac953a0-f9ae-44f0-8d24-edddddda8fa2",
   "metadata": {},
   "source": [
    "We can check if the values are unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85271f65-2fc0-4445-8fa7-19c20ead6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90acc8-afdc-46ce-9293-4ea9b2ebe7c2",
   "metadata": {},
   "source": [
    "With NumPy we can perform arithmetic operations element-wise between arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24956ba-6b41-44af-9183-75fcfe7b4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 1, 1]) + np.array([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335ffb0-9c64-4c0f-997f-4e8e67d97250",
   "metadata": {},
   "source": [
    "Pandas supports this as well, and the index determines how element-wise operations are performed. With addition, only the matching indices are summed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ca9ca-23bd-40c0-9ceb-069ec030c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = np.linspace(0, 10, num=5) # makes numpy array([0, 2.5, 5, 7.5, 10])\n",
    "x = pd.Series(numbers) # index is [0, 1, 2, 3, 4]\n",
    "y = pd.Series(numbers, index=pd.Index([1, 2, 3, 4, 5]))\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9137190-1232-41c4-8c21-bcfec3b042fb",
   "metadata": {},
   "source": [
    "We aren't limited to the integer indices of list-like structures, and we can label our rows. The labels can be altered at any time and be things like dates or even another column. In chapter 3, we will discuss how to perform some operations on the index in order to change it. Then, in chapter 4, we will use the index for operations merging data and aggregating it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0478136-bea2-408f-ba50-03b08ab09d1f",
   "metadata": {},
   "source": [
    "#### `DataFrame`\n",
    "\n",
    "Using a `Series` object for each column enhances the NumPy approach, yet challenges persist in sorting by values or extracting full rows.\n",
    "A `DataFrame` provides a tabular representation comprising multiple `Series` objects as columns and a unified `Index` object labeling the rows.\n",
    "We can construct a `DataFrame` from either of the previously discussed NumPy formats.\n",
    "While it's possible to create a `Series` object for each column, it's unnecessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a2d17-e295-4dbd-8f1f-4012f548c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(array_dict) \n",
    "\n",
    "# this will also work with the first representation\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075d3db-37d2-4158-b356-44b022b92d5e",
   "metadata": {},
   "source": [
    "We can check the type of the underlying data with `dtypes` (note that it is not `dtype` as with `Series` and `Index` objects since each column will have its own data type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafa3a7-85d2-4cfd-bf6c-bea6b94eed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed82b8e-69bf-4124-9cfe-c9e68fd0fd5f",
   "metadata": {},
   "source": [
    "We can get the underlying data with the `values` attribute. Note that this looks very similar to our initial NumPy representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4431cf4-362e-42ce-8c8c-9ee5e26a802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66189f0a-336e-4a9a-8cce-78e8696ae696",
   "metadata": {},
   "source": [
    "We can isolate the columns with the `columns` attribute. Notice that the columns are actually an `Index` object just on a different axis (columns are the horizontal index while rows are the vertical index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc1ca3-3748-40c3-b539-3231ceb9015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a56716-87a8-4db2-a5ed-452a20fcf8ed",
   "metadata": {},
   "source": [
    "Here are some commonly used attributes:\n",
    "\n",
    "|Attribute | Returns |\n",
    "| --- | --- |\n",
    "| `dtypes` | The data types of each column |\n",
    "| `shape` | Dimensions of the `DataFrame` object in a tuple of the form `(number of rows, number of columns)` |\n",
    "| `index` | The `Index` object along the rows of the `DataFrame` object |\n",
    "| `columns` | The name of the columns (as an `Index` object) |\n",
    "| `values` | The data in the `DataFrame` object |\n",
    "| `empty` | Check if the `DataFrame` object is empty |\n",
    "\n",
    "The `Index` object along the rows of the dataframe can be accessed via the `index` attribute (just as with `Series` objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28429812-d4ec-4d32-bcb2-996a63a36400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089e141-0bfe-4632-adec-7ea03aead2c5",
   "metadata": {},
   "source": [
    "As with both `Series` and `Index` objects, we can get the dimensions of the dataframe with the `shape` attribute. The result is of the form `(nrows, ncols)`. Our dataframe has 5 rows and 6 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbe133-ead3-4df1-bec2-343ca2095bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa971a7a-1274-43a8-a053-e33f16fcc084",
   "metadata": {},
   "source": [
    "Pandas allows arithmetic operations on dataframes, matching both index and column for execution.\n",
    "This example showcases addition.\n",
    "In string columns (`time`, `place`, `magType`, and `alert`), pandas concatenates values across dataframes.\n",
    "For numeric columns (`mag` and `tsunami`), the values are summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402961e-aef0-4078-899c-2679a127b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df + df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e9db9-f64a-450e-8da4-ef8382964aa3",
   "metadata": {},
   "source": [
    "### Creating `DataFrame` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579e952-10ca-41e4-b71a-4b424030abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e689c8-7284-4687-a575-f976b18bddb6",
   "metadata": {},
   "source": [
    "#### Creating a `Series` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85744c0-1a3b-4d7a-99b7-722ad0aa43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # set a seed for reproducibility\n",
    "pd.Series(np.random.rand(5), name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2079f44-2587-4c71-a6c9-a8b5568c00a2",
   "metadata": {},
   "source": [
    "#### Creating a `DataFrame` object from a `Series` object\n",
    "Use the `to_frame()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270adc1-421a-4b6c-a8cc-1378440de20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.linspace(0, 10, num=5)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5a314-908b-4a7c-ae53-14aba193579e",
   "metadata": {},
   "source": [
    "#### Creating a `DataFrame` from Python Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab2524-8656-4147-8027-003f027f11ba",
   "metadata": {},
   "source": [
    "##### From a dictionary of list-like structures\n",
    "\n",
    "The dictionary values can be lists, NumPy arrays, etc. as long as they have length (generators don't have length so we can't use them here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276aca56-ac26-42cf-a777-a384400d07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # set seed so result is reproducible\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'random': np.random.rand(5),\n",
    "        'text': ['hot', 'warm', 'cool', 'cold', None],\n",
    "        'truth': [np.random.choice([True, False]) for _ in range(5)]\n",
    "    }, \n",
    "    index=pd.date_range(\n",
    "        end=dt.date(2019, 4, 21),\n",
    "        freq='1D',\n",
    "        periods=5, \n",
    "        name='date'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b76bd-b52b-46a6-9196-bab36ceeacaa",
   "metadata": {},
   "source": [
    "##### From a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40cc1c-f003-4959-9258-3517df72ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    {'mag': 5.2, 'place': 'California'},\n",
    "    {'mag': 1.2, 'place': 'Alaska'},\n",
    "    {'mag': 0.2, 'place': 'California'},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cd8bf-2f7a-4464-8ebe-2e4ac9d51ca1",
   "metadata": {},
   "source": [
    "##### From a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72713c0-5a24-4985-b031-616c2806fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tuples = [(n, n**2, n**3) for n in range(5)]\n",
    "list_of_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93eede-5142-4ce6-acb5-f0d77b23a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    list_of_tuples, \n",
    "    columns=['n', 'n_squared', 'n_cubed']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf31692-8b8c-4310-a477-8b6a70b0c5b9",
   "metadata": {},
   "source": [
    "##### From a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b83d2-99f2-421b-b982-0b73034ed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    np.array([\n",
    "        [0, 0, 0],\n",
    "        [1, 1, 1],\n",
    "        [2, 4, 8],\n",
    "        [3, 9, 27],\n",
    "        [4, 16, 64]\n",
    "    ]), columns=['n', 'n_squared', 'n_cubed']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f9ce3-7dbe-44ed-8af7-d3fd4adf9aa6",
   "metadata": {},
   "source": [
    "### Creating a `DataFrame` object from the contents of a CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafe748-4202-4bb3-89a2-19962b735c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Finding information on the file before reading it in\n",
    "Before attempting to read in a file, we can use the command line to see important information about the file that may determine how we read it in. We can run command line code from Jupyter Notebooks by using `!` before the code.\n",
    "\n",
    "For example, we can find out how many lines are in the file by using the `wc` utility (word count) and counting lines in the file (`-l`). The file has 9,333 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e6c34-ce3f-4b5e-b83b-4a1edc490db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/earthquakes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a6952-60d3-468d-8c92-529ee861dd82",
   "metadata": {},
   "source": [
    "**Windows users**: if the above doesn't work for you (depends on your setup), then use this instead:\n",
    "\n",
    "```python\n",
    "!find /c /v \"\" data\\earthquakes.csv\n",
    "```\n",
    "\n",
    "\n",
    "We can find the file size by using `ls` to list the files in the `data` directory, and passing in the flags `-lh` to include the file size in human readable format. Then we use `grep` to find the file in question. Note that `|` passes the result of `ls` to `grep`. The `grep` utility is used for finding items that match patterns.\n",
    "\n",
    "This tells us the file is 3.4 MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c024ae1-8f94-4752-92c9-7c71082adc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh data | grep earthquakes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6ffa8-6d8c-475d-b5d3-8265d74de456",
   "metadata": {},
   "source": [
    "**Windows users**: if the above doesn't work for you (depends on your setup), then use this instead:\n",
    "\n",
    "```python\n",
    "!dir data | findstr \"earthquakes.csv\"\n",
    "```\n",
    "\n",
    "We can even capture the result of a command and use it in our Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35aeff-f7de-49d3-b917-0b3fb1e62f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !ls -lh data\n",
    "[file for file in files if 'earthquake' in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9918a-aae0-4152-8650-2fbe8f925218",
   "metadata": {},
   "source": [
    "**Windows users**: if the above doesn't work for you (depends on your setup), then use this instead:\n",
    "\n",
    "```python\n",
    "files = !dir data\n",
    "[file for file in files if 'earthquake' in file]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2fe223-af7a-4b0a-be85-0325c82c11e4",
   "metadata": {},
   "source": [
    "##### Examining a few rows\n",
    "\n",
    "We can use `head` to look at the top `n` rows of the file. With the `-n` flag, we can specify how many. This shows use that the first row of the file contains headers and that it is comma-separated (just because the file extension is `.csv` doesn't it contains comma-separated values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc213fb-e153-4abd-bb85-180bacdf1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 data/earthquakes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76669b2-07f4-4fb4-9ace-7317201ee467",
   "metadata": {},
   "source": [
    "**Windows users**: if the above doesn't work for you (depends on your setup), then use this instead:\n",
    "\n",
    "```python\n",
    "n = 2\n",
    "with open('data/earthquakes.csv', 'r') as file:\n",
    "    for _ in range(n):\n",
    "        print(file.readline(), end='\\r')\n",
    "```\n",
    "\n",
    "\n",
    "Just like `head` gives rows from the top, `tail` gives rows from the bottom. This can help us check that there is no extraneous data on the bottom of the field, like perhaps some metadata about the fields that actually isn't part of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abd901-4049-47b3-a325-34b1025322ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 1 data/earthquakes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdedb86-3fd5-4e8c-9f21-8587319bcc6d",
   "metadata": {},
   "source": [
    "**Windows users**: if the above doesn't work for you (depends on your setup), then use this instead:\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "with open('data/earthquakes.csv', 'rb') as file:\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    while file.read(1) != b'\\n':\n",
    "        file.seek(-2, os.SEEK_CUR)\n",
    "    print(file.readline().decode())\n",
    "```\n",
    "\n",
    "*Note*: To inspect more than one row from the end of the file, you will have to use this instead, which requires reading the whole file:\n",
    "\n",
    "```python\n",
    "n = 2\n",
    "with open('data/earthquakes.csv', 'r') as file:\n",
    "    print('\\r'.join(file.readlines()[-n:]))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c7428-4d23-488e-8ccb-bcf9a3cd1a6f",
   "metadata": {},
   "source": [
    "##### Column count\n",
    "We can use `awk` to find the column count. This is a utility for pattern scanning and processing. The `-F` flag allows us to specify the delimiter (comma, in this case). Then we specify what to do for each record in the file. We choose to print `NF` which is a predefined variable whose value is the number of fields in the current record. Here, we say `exit` so that we print the number of fields (columns, here) in the first row of the file, then we stop. \n",
    "\n",
    "This tells us we have 26 data columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836e4be-8a96-4a71-b353-e5e05857e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk -F',' '{print NF; exit}' data/earthquakes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1456f-bf06-4fb9-b836-97c8bbc8f530",
   "metadata": {},
   "source": [
    "**Windows users**: if the above or below don't work for you (depends on your setup), then use this instead:\n",
    "\n",
    "```python\n",
    "with open('data/earthquakes.csv', 'r') as file:\n",
    "    print(len(file.readline().split(',')))\n",
    "```\n",
    "\n",
    "\n",
    "Since we know the 1st line of the file had headers, and the file is comma-separated, we can also count the columns by using `head` to get headers and parsing them in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1508d86-d650-4b00-941d-07edfd6332d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = !head -n 1 data/earthquakes.csv\n",
    "len(headers[0].split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf72aed-d973-40b1-9c3b-8aae27d51962",
   "metadata": {},
   "source": [
    "**Windows users**: if you had to use the alternatives above, consider trying out [Cygwin](https://www.cygwin.com) or [Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/en-us/windows/wsl/about).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965bc6d-d870-4911-a0da-1c6c9b3361a3",
   "metadata": {},
   "source": [
    "#### Reading in the file\n",
    "\n",
    "Our file is small in size, has headers in the first row, and is comma-separated, so we don't need to provide any additional arguments to read in the file with `pd.read_csv()`, but be sure to check the [documentation](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) for possible arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe84f7-d525-47db-9ead-2c5dbc5e780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/earthquakes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98518e7-cb75-4516-ac60-52aea28e919a",
   "metadata": {},
   "source": [
    "Note that we can also pass in a URL. Let's read this same file from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b72e26-347b-4039-913c-58b1aebc77cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://github.com/stefmolin/'\n",
    "    'Hands-On-Data-Analysis-with-Pandas-2nd-edition'\n",
    "    '/blob/master/ch_02/data/earthquakes.csv?raw=True'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4bece1-e8cd-494e-b38c-448669a72cb9",
   "metadata": {},
   "source": [
    "Pandas is usually very good at figuring out which options to use based on the input data, so we often won't need to add arguments to the call; however, there are many options available should we need them, some of which include the following:\n",
    "\n",
    "| Parameter | Purpose |\n",
    "| --- | --- |\n",
    "| `sep` | Specifies the delimiter |\n",
    "| `header` | Row number where the column names are located; the default option has `pandas` infer whether they are present |\n",
    "| `names` | List of column names to use as the header |\n",
    "| `index_col` | Column to use as the index |\n",
    "| `usecols` | Specifies which columns to read in |\n",
    "| `dtype` | Specifies data types for the columns | \n",
    "| `converters` | Specifies functions for converting data in certain columns |\n",
    "| `skiprows` | Rows to skip |\n",
    "| `nrows` | Number of rows to read at a time (combine with `skiprows` to read a file bit by bit) |\n",
    "| `parse_dates` | Automatically parse columns containing dates into datetime objects |\n",
    "| `chunksize` | For reading the file in chunks |\n",
    "| `compression` | For reading in compressed files without extracting beforehand |\n",
    "| `encoding` | Specifies the file encoding |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e38f10-1679-4988-97dc-60f7f3c00129",
   "metadata": {},
   "source": [
    "#### Writing a `DataFrame` Object to a CSV File\n",
    "\n",
    "Our file is small in size, has headers in the first row, and is comma-separated, so we don't need to provide any additional arguments to read in the file with `pd.read_csv()`, but be sure to check the [documentation](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) for possible arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149c9fd-0893-419e-804d-799f5fff71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73c4cf-7f99-43a3-9fce-5b1848c8a6dc",
   "metadata": {},
   "source": [
    "### Writing a `DataFrame` Object to a Database\n",
    "Note the `if_exists` parameter. By default, it will give you an error if you try to write a table that already exists. Here, we don't care if it is overwritten. Lastly, if we are interested in appending new rows, we set that to `'append'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1c022-e8f1-417a-ad49-c3584386224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "with sqlite3.connect('data/01/quakes.db') as connection:\n",
    "    pd.read_csv('data/01/tsunamis.csv').to_sql(\n",
    "        'tsunamis', connection, index=False, if_exists='replace'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241638c-a31e-4b3c-9d08-690287a850f0",
   "metadata": {},
   "source": [
    "### Creating a `DataFrame` Object by Querying a Database\n",
    "Using a SQLite database. Otherwise you need to install [SQLAlchemy](https://www.sqlalchemy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073277f4-f497-43ea-a230-7d8a33de88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "with sqlite3.connect('data/01/quakes.db') as connection:\n",
    "    tsunamis = pd.read_sql('SELECT * FROM tsunamis', connection)\n",
    "\n",
    "tsunamis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f58970-bfca-469f-a6d8-8155b2803f73",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b07308-e1eb-49c1-82a9-282fc93be204",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fae89cbd-0b6d-4e26-ab74-39113f9c7a3b",
   "metadata": {},
   "source": [
    "## Data aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac245fbb-f138-4ff2-ae64-bdcaa89f67ed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
