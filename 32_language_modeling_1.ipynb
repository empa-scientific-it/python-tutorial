{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Language Modeling With PyTorch\n",
    "\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [References](#References)\n",
    "- [Inspecting the data](#Inspecting-the-data)\n",
    "- [Bigram language model](#Bigram-language-model)\n",
    "  - [Evaluating the quality of the model](#Evaluating-the-quality-of-the-model)\n",
    "- [A neural network approach](#A-neural-network-approach)\n",
    "  - [The training set](#The-training-set)\n",
    "  - [Feeding the network](#Feeding-the-network)\n",
    "  - [Regaining a normal distribution](#Regaining-a-normal-distribution)\n",
    "  - [Recap: How the Neural Network Processes Input Characters](#Recap:-How-the-Neural-Network-Processes-Input-Characters)\n",
    "  - [Optimization](#Optimization)\n",
    "  - [Putting it all together](#Putting-it-all-together)\n",
    "    - [Preparing data](#Preparing-data)\n",
    "    - [Initializing the neural network](#Initializing-the-neural-network)\n",
    "    - [Training the neural network](#Training-the-neural-network)\n",
    "    - [Comparison with a Bigram frequency model](#Comparison-with-a-Bigram-frequency-model)\n",
    "    - [Smoothing applied to a neural network](#Smoothing-applied-to-a-neural-network)\n",
    "    - [Sampling from our trained model](#Sampling-from-our-trained-model)\n",
    "  - [Conclusion](#Conclusion)\n",
    "- [Exercises](#Exercises)\n",
    "  - [1. Build a Trigram model](#1.-Build-a-Trigram-model)\n",
    "  - [2. Split the dataset](#2.-Split-the-dataset)\n",
    "    - [Bigram model baseline](#Bigram-model-baseline)\n",
    "    - [Compare the Bigram and Trigram model](#Compare-the-Bigram-and-Trigram-model)\n",
    "  - [3. Change the loss function](#3.-Change-the-loss-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "The purpose of this multi-part notebook is to give a gentle introduction to the PyTorch library, with a focus on language modeling.\n",
    "At high-level, we will build a progressively more complex **character-level language model** that can generate more text similar to the training data.\n",
    "\n",
    "The final result is not meant to be a \"production-ready\" language model, but rather a simple yet effective example of how to use PyTorch for language modeling. Along the way, we will learn the fundamental building blocks that lay the groundwork for more complex models, including the base models that powers the state-of-the-art LLMs and derived products, like our friendly and always helpful assistant ChatGPT.\n",
    "\n",
    "The final implementation will allow you to experiment with different models, starting from the most simple and basic one (a **bigram** model) to a more complex **RNN** and finally a **Transformer** model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Some literature references about the concepts touched by one or more parts of this tutorial: \n",
    "\n",
    "- Bigram (one character predicts the next one with a lookup table of counts)\n",
    "- Multi-Layer Perceptron (MLP): [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "- Convolutional Neural Network (CNN): [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499)\n",
    "- Recurrent Neural Network (RNN): [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    "- Long Short-Term Memory (LSTM): [Graves et al. 2014](https://arxiv.org/abs/1308.0850)\n",
    "- Gated Recurrent Unit (GRU): [Kyunghyun Cho et al. 2014](https://arxiv.org/abs/1409.1259)\n",
    "- Transformer: [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "A few more related resource (hands-on, tutorial, articles, videos, etc.):\n",
    "- Book \"[Build a Large Language Model (From Scratch)](http://mng.bz/orYv)\" by Sebastian Raschka (the companion [GitHub repository](https://github.com/rasbt/LLMs-from-scratch))\n",
    "- [Andrej Karpathy's \"Neural Net: From Zero to Hero\"](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) (*This was the **main** inspiration for this and the subsequent notebooks*)\n",
    "- A [tutorial](https://docs.fast.ai/tutorial.text.html) on *transfer learning* by fastai\n",
    "- [Hugging Face's FineWeb dataset](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)\n",
    "- [Transformer LLM 3D visualizer](https://bbycroft.net/llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Our initial dataset is a simple list of strings that represent common names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pl.Path(\"data/names.txt\").read_text().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words), min(len(w) for w in words), max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The information we can extract from a single name, e.g. `isabella`, is multiple:\n",
    "\n",
    "- We know that the character `i` is followed by `s`.\n",
    "- We know that, after the characters `isabell`, the following character is `a`.\n",
    "- We know that, after the characters `isabella`, the following character is `\\n` (end of string).\n",
    "\n",
    "The idea is that a single word packs multiple pieces of information regarding the statistical structure of the language it belongs to.\n",
    "And since we have about 32k words, there's quite a lot of information we can use to train even a simple language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Bigram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "A bigram language model is the simplest possible language model.\n",
    "Given a sequence of characters (each character is usually referred to as a **token**), the bigram language model assigns a probability to each possible next token, given the previous token.\n",
    "It's a predictor for each pair of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:3]:\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The most basic modeling of the statistical patterns embedded in our input data is predidicting the next token **by frequency**.\n",
    "We can build a simple dictionary that counts how many times a bigram (i.e., sequence of two tokens) appear in our dataset.\n",
    "\n",
    "We also need to add the *special* information about the start and end of the sequence.\n",
    "We can \"encode\" that information with two **special tokens**: `<S>` (start) and `<E>` (end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#% Student code\n",
    "from collections import defaultdict\n",
    "\n",
    "bigrams = defaultdict(int)\n",
    "\n",
    "for w in words:\n",
    "    chars = [\"<S>\"] + list(w) + [\"<E>\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        bigrams[(ch1, ch2)] += 1\n",
    "\n",
    "print(dict(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "What does an entry of the bigram dictionary look like?\n",
    "It's something like `('a', '<E>'): 6640`, which means: the bigram `('a', '<E>')` occurred 6640 times.\n",
    "That is: the letter `a` is quite likely to appear at the end of a name.\n",
    "\n",
    "We want to sort the bigrams by their count, from the most frequent to the least frequent.\n",
    "Let's see the first 10 most frequent bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(bigrams.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "A much better way to store this information in a 2D array, where the rows are going to be the 1st character and the columns are going to be the 2nd character.\n",
    "The entry at row `a` and column `b` is going to be the count of the bigram `ab`.\n",
    "\n",
    "Since we are dealing with 26 characters, plus `<S>` and `<E>`, we need a total of 28x28 = 784 entries.\n",
    "Bracketed tokens are customary in NLP to represent special tokens, but here we are only interested in knowing when a sentence starts or ends.\n",
    "\n",
    "But there's also a problem with keeping two special tokens for the start and end of a sentence: we can't have `('<S>', '<S>')` or `('<E>', '<E>')`, or other combinations like `('a', '<S>')` or `('<E>', 'b')`.\n",
    "These would be invalid bigrams.\n",
    "To solve this, we can replace `<S>` with `.` and `<E>` with `.` and have a total of 27x27 = 729 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "How should we encode the bigrams?\n",
    "Our 2D array is going to hold integers **only**, so we need a way to make this conversion.\n",
    "One way is to be a so-called **vocabulary** from our input data.\n",
    "\n",
    "A vocabulary requires two functions:\n",
    "1. `stoi`: string to integer (**encoding**)\n",
    "2. `itos`: integer to string (**decoding**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(''.join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0 # special token for end of sentence is mapped to 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#% Student code\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    i = stoi[ch1]\n",
    "    j = stoi[ch2]\n",
    "    N[i, j] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Let's create a nice visualization of our bigram frequency table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "norm = mpl.colors.Normalize(vmin=N.min(), vmax=N.max())\n",
    "im = ax.imshow(N, cmap='Blues')\n",
    "\n",
    "for i in range(N.shape[0]):\n",
    "    for j in range(N.shape[1]):\n",
    "        val = norm(N[i,j])\n",
    "        text_color = 'white' if val > 0.5 else 'black'\n",
    "        # character\n",
    "        ax.text(j, i, itos[i]+itos[j],\n",
    "                ha=\"center\", va=\"bottom\",\n",
    "                color=text_color, fontweight='bold')\n",
    "        # count\n",
    "        ax.text(j, i, int(N[i,j]),\n",
    "                ha=\"center\", va=\"top\",\n",
    "                color=text_color)\n",
    "\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Our bigram model is essentially an iterative sampling from a probability distribution that describes how frequent each bigram is in the dataset.\n",
    "\n",
    "We already have the frequency table, so we need to built a probability distribution and a sampling mechanism.\n",
    "Let's do it for the first row, which represents the frequency of each bigram starting with the character '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = N[0].float()\n",
    "proba /= proba.sum()\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "PyTorch provides us with a method to sample from a [**Multinomial distribution**](https://docs.pytorch.org/docs/main/distributions.html#multinomial).\n",
    "A Multinomial distribution is a generalization of a [**Binomial distribution**](https://en.wikipedia.org/wiki/Binomial_distribution), where we sample from a distribution with more than two outcomes.\n",
    "\n",
    "To enforce predictability, we can initialize a random number generator with a fixed seed.\n",
    "Also, we need to allow sampling **with replacement**, so that we can sample the same token multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(proba, num_samples=1, replacement=True, generator=gen).item()\n",
    "print(itos[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "We can of course sample as many tokens as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(proba, num_samples=100, replacement=True, generator=gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "You might have understood how the process goes: after we extract a given bigram, we need to lookup the most likely bigram that starts with the second character of the first bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "NUM_WORDS = 20\n",
    "\n",
    "for _ in range(NUM_WORDS):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "\n",
    "  while True:\n",
    "    # Compute the probabilities\n",
    "    p = N[ix].float()\n",
    "    p /= p.sum()\n",
    "    \n",
    "    # Sample the next character\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    \n",
    "    # Add the character to the output\n",
    "    out.append(itos[ix])\n",
    "    \n",
    "    # Stop if we reach the end of the text\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "The results are quite terrible, although they're reasonable given the simplicity of the model and the patterns we're trying to capture.\n",
    "\n",
    "The core problem is that a bigram model looks only the the frequency of a pair of tokens, but it has zero information of what's most likely to come before or after those two tokens.\n",
    "You can imagine that the obvious next step is a **trigram** model, which looks at the frequency of a triplet of tokens.\n",
    "\n",
    "Let's now improve a bit our code: the first thing is to compute **all** the probabilities once, and then sample from them.\n",
    "PyTorch tensors support **vectorized** operations, which means that we can perform operations on entire tensors at once, without having to loop through them.\n",
    "\n",
    "Each row of our 2D matrix contains the counts of how many times the token with that row index is followed by all the other tokens, whose indexes run along the columns.\n",
    "\n",
    "$$\n",
    "P_{ij}= \\frac{N_{ij}}{\\displaystyle\\sum_{k} N_{i k}}\n",
    "$$\n",
    "\n",
    "For each pair $(i,j)$:\n",
    "- The numerator $N_{ij}$ is the count of the number of times token `j` follows token `i`.\n",
    "- The denominator $\\sum_{k} N_{i k}$ is the total number of times *any* character follows `i`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "In Python\n",
    "\n",
    "```python\n",
    "P = N.float()\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Here `dim=1` tells PyTorch to sum over the columns (the second index), while `keepdim=True` tells it to keep the first dimension (the first index) as a singleton (a `1`) dimension.\n",
    "Without `keepdim=True`, the result would have shape `(27,)`, and performing the division would produce the wrong result because of how [brodcasting](https://pytorch.org/docs/stable/notes/broadcasting.html) works.\n",
    "\n",
    "> Try to experiment with the `keepdim` parameter and see what happens if you remove it.\n",
    "> Can you explain why the predictions become complete garbage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "P = N.float()\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "\n",
    "for _ in range(10):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "\n",
    "  while True:\n",
    "    # Get the probabilities\n",
    "    p = P[ix]\n",
    "    \n",
    "    # Sample the next character\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    \n",
    "    # Add the character to the output\n",
    "    out.append(itos[ix])\n",
    "    \n",
    "    # Stop if we reach the end of the text\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Evaluating the quality of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "We have built a bigram language model by counting letter combination frequencies, then normalizing and sampling with that probability base.\n",
    "\n",
    "We trained the model, we sampled from the model (iteratively, character-wise). But its still bad at coming up with names.\n",
    "\n",
    "But how bad? We know that the model's \"knowledge\" is represented by `P`, but how can we boil down the model's quality in one value?\n",
    "\n",
    "First, let's look at the bigrams we created from the dataset: the bigrams to `emma` are `.e, em, mm, ma, a.`.\n",
    "**What probability does the model assign to each of those bigrams?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # Neat way for two char 'sliding-window'\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        print(f'{ch1}{ch2}: {prob:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Anything above or below $\\frac{1}{27} \\approx 3.7\\%$ means we deviate from the mean, that is, a completely uniform distribution of bigrams. \n",
    "And that means we learned something from the bigram statistics.\n",
    "\n",
    "How can we summarize these probabilities into a quality indicating measurement?\n",
    "We may compute the product of all probabilities — a number called the **likelihood**.\n",
    "But since all these probabilities are small numbers, the product is also a small number, and it is hard to compare likelihoods.\n",
    "Solution: *The log-likelihood, the **sum** of $\\log(P)$ over all the individual token probabilities* ($\\log$ is applied for convenience).\n",
    "\n",
    "> The higher the log-likelihood, the better the model, because the more capable it is of predicting the next character in a sequence from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "log_likelihood = 0.0\n",
    "n = 0  # character pair count\n",
    "\n",
    "for word in words:\n",
    "    # Add start/end tokens and convert to character list\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    \n",
    "    # Calculate log probabilities in a more compact way\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        prob = P[stoi[ch1], stoi[ch2]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        n += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "\n",
    "nll = -log_likelihood\n",
    "\n",
    "print(f'{nll=}')  # Negative log likelihood\n",
    "print(f'Average NLL: {nll/n:.4f}')  # More descriptive output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "We calculated a negative log-likelihood, because this follows the convention of setting the goal to minimize the **loss function**, the function that drives the optimization (i.e., training) process.\n",
    "The lower the loss/negative log-likelihood, the better the model.\n",
    "\n",
    "We got $2.45$ for the model. The lower, the better.\n",
    "We need to find the parameters that reduce this value.\n",
    "\n",
    "**Goal:** Maximize likelihood of the trained data w. r. t. model parameters in `P`\n",
    "- This is equivalent to maximizing the log-likelihood (as $\\log$ is monotonic)\n",
    "- This is equivalent to minimizing the *negative* log-likelihood\n",
    "- And this is equivalent to minimizing the average negative log-likelihood (the quality-measurement, as shown by $2.45$ above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "There's an immediate problem, though: if we have a word containing a bigram that **never** appears in our training data, the model will assign a probability of $0$ to it, which will make the log-likelihood $-\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "log_likelihood = 0.0\n",
    "n = 0  # character pair count\n",
    "\n",
    "for word in [\"edobq\"]:\n",
    "    # Add start/end tokens and convert to character list\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    \n",
    "    # Calculate log probabilities in a more compact way\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        prob = P[stoi[ch1], stoi[ch2]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        n += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')  # Negative log likelihood\n",
    "print(f'Average NLL: {nll/n:.4f}')  # More descriptive output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "A negative infinite log-likelihood is definitely not good because our optimizer will never find a \"stable\" solution.\n",
    "\n",
    "One simple fix is to assign a small but non-zero probability to every bigram: this is called **model smoothing**.\n",
    "The easiest way is to ensure that no bigram *never* appears: we can achieve this by adding a constant to our 2D matrix `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS = (N + 1).float()  # The higher the number, the more smoothing we apply\n",
    "PS /= PS.sum(dim=1, keepdim=True)\n",
    "\n",
    "# Initialize variables\n",
    "log_likelihood = 0.0\n",
    "n = 0  # character pair count\n",
    "\n",
    "for word in [\"edobq\"]:\n",
    "    # Add start/end tokens and convert to character list\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    \n",
    "    # Calculate log probabilities in a more compact way\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        prob = PS[stoi[ch1], stoi[ch2]]  # Use the smoothed probabilities\n",
    "        log_likelihood += torch.log(prob)\n",
    "        n += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')  # Negative log likelihood\n",
    "print(f'Average NLL: {nll/n:.4f}')  # More descriptive output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## A neural network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "We will cast the problem of character estimation into the framework of neural networks.\n",
    "The problem remains the same, the approach changes, and the outcome should look similar.\n",
    "\n",
    "Our neural network **receives a single character** and **outputs the probability distribution over the next possible characters** ($27$ in this case).\n",
    "\n",
    "It's going to make guesses on the most likely character to follow.\n",
    "We can still measure the performance through the *same* loss function, the negative log-likelihood.\n",
    "\n",
    "From the training data, we also know the character that actually comes next in each training example.\n",
    "We'll use this information to fine-tune (i.e., train or update the parameters of) the neural network to make better guesses: this is a textbook example of **supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### The training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training set of all bigrams\n",
    "xs, ys = [], [] # Input and output character indices\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "# Convert lists to tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "We can inspect the first few tokens from the two tensors:\n",
    "- `xs` will be the input tokens (what the model will see)\n",
    "- `ys` will be the target tokens (what we want the model to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f'For character #{i} \"{itos[xs[i].item()]}\" in xs, we expect the model to predict \"{itos[ys[i].item()]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "One important detail about PyTorch tensors: there exists `torch.Tensor` (a class) and `torch.tensor()` (a method).\n",
    "They are related, but different:\n",
    "- `torch.Tensor` is a class, and every PyTorch tensor is an instance of this class.\n",
    "- `torch.tensor()` is a method to create a tensor, with `dtype` automatically inferred from the input data.\n",
    "\n",
    "Except for when initializing a completely empty tensor, in general there is no reason to choose `torch.Tensor` over `torch.tensor`. \n",
    "Note that `torch.Tensor` is an alias for `torch.FloatTensor`, with a default `dtype` of `torch.float32`.\n",
    "\n",
    "In general, you should use `torch.tensor()` almost always, unless you have a specific reason to use `torch.Tensor`.\n",
    "\n",
    "Here you can find more details from the official docs:\n",
    "- [`torch.tensor()`](https://docs.pytorch.org/docs/stable/generated/torch.tensor.html#torch-tensor)\n",
    "- [`torch.Tensor`](https://docs.pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.dtype, xs.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "`torch.LongTensor` here means that the tensor contains 64-bit (8-byte) integers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Feeding the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "A neural network essentially is made up of **layers**.\n",
    "At high-level, each layer is typically a linear transformation followed by a non-linear activation function.\n",
    "Something like: $\\text{output} = \\text{activation}(\\text{weights} \\cdot \\text{input} + \\text{bias})$\n",
    "\n",
    "If we were to feed our characters as integer indexes, we would have a sequence of integer indexes as input.\n",
    "If `a` is 1 and `z` is 25, the weight applied to `z` will have 25 times more impact on the output than `a`. This creates an arbitrary and misleading mathematical relationship.\n",
    "\n",
    "Moreover, during the training (optimization) phase, the updates to the weights will be proportional to their input values.\n",
    "Larger input values will cause larger updates to the weights, which can lead to unstable training.\n",
    "\n",
    "And lastly, the network has no reference to the potential value range. It doesn't know that the values are constrained to a specific set (like 0-25 for letters).\n",
    "\n",
    "To address all these issues, we can use **one-hot encoding**.\n",
    "One-hot encoding each letter means creating a vector where only one position has a value of 1 (corresponding to that letter's position in the alphabet) and all other positions are 0.\n",
    "This gives the neural network a much clearer signal about which letter is present without introducing misleading numerical relationships.\n",
    "This approach is particularly important in language models where the relationships between symbols (letters, words) are learned from their contexts and co-occurrences, not from arbitrary numeric values assigned to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Luckily, PyTorch provides a convenient way to perform one-hot encoding using the `torch.nn.functional.one_hot` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Let's encode the first 5 tokens (`.emma`), print and visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.one_hot(xs[:5], num_classes=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs[:5], num_classes=27)\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "One problem is the `dtype` of the one-hot encoded tensor.\n",
    "It's `torch.int64` by default (inferred from our data), but we need `torch.float32` to have a the input suitable for the mathematical operations the network will perform.\n",
    "\n",
    "We can convert it using `.float()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs[:5], num_classes=27).float()\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Let's experiment with neurons.\n",
    "We build one $27$-dimensional neuron and approach it with the letter-wise input of our first name `.emma` (That's $5$ letters)\n",
    "\n",
    "A neuron is represented as a column vector of $27$ numbers, randomly drawn from a normal distribution.\n",
    "We can do this with [`torch.randn`](https://docs.pytorch.org/docs/main/generated/torch.randn.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "W = torch.randn((27,1), generator=g)\n",
    "# '@' is PyTorch's matrix multiplication operator (5x27 @ 27x1 -> 5x1)\n",
    "a = xenc @ W\n",
    "\n",
    "# This is now a 5x1 vector\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "`W` is a **single** neuron.\n",
    "Multiplying it by `xenc` makes it 'react' to the one-hot encoded input. The result is a $5\\times 1$ vector.\n",
    "\n",
    "`.emma` has $5$ characters, we have $1$ neuron.\n",
    "When this neuron processes the 5 characters of \".emma\", it produces 5 activation values, one for each character.\n",
    "Each activation value represents how strongly the neuron \"reacts\" to or \"recognizes\" each character.\n",
    "\n",
    "We want to have $27$ neurons, one for each possible character.\n",
    "Each neuron can \"specialize\" in recognizing one specific character.\n",
    "Also, since each neuron has $27$ dimensions, we'll end up with a matrix $27\\times 27$ of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27,27), generator=g)\n",
    "a = xenc @ W\n",
    "\n",
    "# This is now a 5x27 matrix\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "What the matrix multiplication did is give us the **firing rates** of each neuron for each character.\n",
    "For example `(a @ W)[3, 13]` is the firing rate of the 14th neuron for the 4th character.\n",
    "This is a very efficient way of computing the firing rates for all characters and all neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc[3], W[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc[3] * W[:, 13]).sum(), (xenc @ W)[3, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### Regaining a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "We want the neurons per input (per character) to come up with a $27$-dimensional activation of values that could be transformed into a normal distribution on what character to choose next.\n",
    "We've seen that with the Bigram's probability distribution, given info per character on what character ist most likely to follow.\n",
    "\n",
    "Right now, for every character we get $27$ numbers, positive and negative, but not following a normal distribution.\n",
    "\n",
    "- **What we want:** For each character in our input, we want to predict the probability distribution of the next character (similar to a bigram model).\n",
    "\n",
    "- **What we have:** For each character in our input, we get $27$ numbers, positive and negative, but not following a normal distribution.\n",
    "\n",
    "- **Problem:** These raw outputs are just arbitrary numbers that can be positive or negative.\n",
    "They don't naturally sum to 1 or represent probabilities.\n",
    "\n",
    "The solution is to change our interpretation of the output values of the neurons.\n",
    "We can think of the neural network's raw outputs as \"log-counts\" (also called \"logits\") or \"scores\" rather than direct probabilities.\n",
    "\n",
    "To convert this numbers into proper probabilities:\n",
    "\n",
    "1. First, we **exponentiate** each value (turning negative numbers into small positives and making large positives even larger)\n",
    "2. Then, we normalize the values so that they sum to 1.\n",
    "\n",
    "This is achievied with a function called **softmax** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the raw output scores (logits) for each character in the input\n",
    "logits = xenc @ W  # Shape: (5, 27) - 5 input characters, 27 possible output characters\n",
    "\n",
    "# Exponentiate the logits to get positive \"fake counts\" (similar to unnormalized probabilities)\n",
    "counts = logits.exp()  # Negative logits yield values between 0 and 1; positive logits yield values greater than 1\n",
    "\n",
    "# Normalize the counts along each row to obtain probabilities (each row sums to 1)\n",
    "probs = counts / counts.sum(1, keepdims=True)  # Shape: (5, 27)\n",
    "\n",
    "# Print the shape of the probability matrix and verify normalization\n",
    "print(probs.shape)      # Will print: (5, 27)\n",
    "print(probs[0].sum())   # Will print: 1.0 (or very close, due to floating point precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "It might seem unusual, but after this transformation, we have a set of numbers that we can use just like the actual counts from the bigram model.\n",
    "\n",
    "All the values are non-negative—think of them as \"pseudo-counts.\"\n",
    "Now, our goal is simply to adjust the weights `W` so that the network produces the correct character indices as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Recap: How the Neural Network Processes Input Characters\n",
    "\n",
    "Given the input `.emma`, the neural network handles each character step by step:\n",
    "\n",
    "1. **Input Preparation:**  \n",
    "   - Take the current character (e.g., `.`) and map it to its index (e.g., 0).\n",
    "   - One-hot encode this index into a 27-dimensional vector.\n",
    "\n",
    "2. **Neural Network Computation:**  \n",
    "   - Feed the one-hot vector into the network (shape: $27 \\times 1$).\n",
    "   - The network applies its weights, producing a $1 \\times 27$ vector of activations (logits).\n",
    "\n",
    "3. **Softmax Transformation:**  \n",
    "   - Exponentiate each logit to ensure all values are positive.\n",
    "   - Normalize the result so the values sum to 1, yielding a probability distribution over all possible next characters.\n",
    "\n",
    "The Softmax function turns the network's raw outputs into probabilities, indicating how likely each character is to follow the current input.\n",
    "\n",
    "The core question is now:  \n",
    "Can we optimize the weights `W` so that the network’s predicted probabilities match the actual sequence in our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "We can now evaluate how well our neural network predicts the next character in a sequence. For each bigram (pair of consecutive characters), we:\n",
    "\n",
    "- Feed the input character to the neural network.\n",
    "- Get the predicted probability distribution for the next character.\n",
    "- Check how much probability the network assigns to the actual next character.\n",
    "- Calculate the negative log-likelihood (NLL) for this prediction, which tells us how \"surprised\" the network is by the true answer (lower is better).\n",
    "- Finally, we average the NLLs across all bigrams to get the overall loss, which is what we want to minimize during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder: xs is a tensor containing the indexes of **all** the characters\n",
    "# We take the first 5 characters to form the bigrams\n",
    "xs = xs[:5]\n",
    "ys = ys[:5]\n",
    "\n",
    "# Initialize tensor to store negative log likelihoods (NLL) for each bigram\n",
    "nlls = torch.zeros(len(xs))  # There are 5 bigrams in '.emma.'\n",
    "\n",
    "# Loop through each bigram in the sequence\n",
    "for i in range(len(xs)):\n",
    "    x = xs[i].item()  # Input character index\n",
    "    y = ys[i].item()  # Target (next) character index\n",
    "\n",
    "    print(\"\\n-------\")\n",
    "    print(f'Bigram {i+1}: (\"{itos[x]}\", \"{itos[y]}\") [indexes ({x}, {y})]')\n",
    "    print(f'  Input to neural net: {x} (\"{itos[x]}\")')\n",
    "    print(f'  Output probabilities:\\n{probs[i]}')\n",
    "    \n",
    "    # Most likely next character according to the model\n",
    "    predicted_index = probs[i].argmax().item()\n",
    "    predicted_char = itos[predicted_index]\n",
    "    predicted_prob = probs[i].max().item()\n",
    "    print(f'  Most likely next character: {predicted_char} (index {predicted_index}, probability {predicted_prob:.4f})')\n",
    "    \n",
    "    print(f'  Actual next character (label): {itos[y]} (index {y})')\n",
    "    \n",
    "    # Probability assigned to the correct character\n",
    "    p = probs[i, y]\n",
    "    print(f'  Probability assigned to correct character: {p.item():.4f}')\n",
    "    \n",
    "    # Log likelihood and negative log likelihood\n",
    "    logp = torch.log(p)\n",
    "    nll = -logp\n",
    "    print(f'  Log likelihood: {logp.item():.4f}')\n",
    "    print(f'  Negative log likelihood: {nll.item():.4f}')\n",
    "    \n",
    "    # Store NLL for this bigram\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('\\n============')\n",
    "print(f'Average negative log likelihood (loss): {nlls.mean().item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "What do these results tell us?\n",
    "- In each case, the probability assigned to the correct next character is relatively low, meaning the model is not yet confident in its predictions.\n",
    "- The most likely character predicted by the model is often not the correct one.\n",
    "- The negative log likelihood values are relatively high, indicating the model is “surprised” by the true next character.\n",
    "- The final line reports the average negative log likelihood (loss) across all bigrams: 3.44. This is a key metric for training—the goal is to minimize this value by adjusting the model’s weights.\n",
    "\n",
    "The network is currently not very accurate at predicting the next character, as shown by the low probabilities for the correct answers and the high loss. This is expected at the start, before any training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "Remember that we started with `W` as a completely random matrix of floats.\n",
    "Hoping that random initialization would yield a good solution is like hoping that a random collection of Lego bricks will build a house.\n",
    "\n",
    "Instead, we will actively improve the model’s predictions. Specifically, we will adjust the weights in the matrix `W` to increase the probability of correctly predicting the second character in each bigram.\n",
    "\n",
    "This is done by computing how the loss changes with respect to each weight (i.e., calculating the gradients), and then updating the weights in a way to reduce the overall loss.\n",
    "This process—called **gradient-based optimization**—enables the neural network to learn from its mistakes and become better at predicting the next character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the probability assigned by the model to the correct next character for each bigram\n",
    "bigram_descriptions = [\n",
    "    ('input \".\" → output \"e\"', 0, 5),\n",
    "    ('input \"e\" → output \"m\"', 1, 13),\n",
    "    ('input \"m\" → output \"m\"', 2, 13),\n",
    "    ('input \"m\" → output \"a\"', 3, 1),\n",
    "    ('input \"a\" → output \".\"', 4, 0),\n",
    "]\n",
    "\n",
    "print(\"Probability assigned by the model to the correct next character for each bigram:\\n\")\n",
    "for desc, i, j in bigram_descriptions:\n",
    "    prob = probs[i, j].item()\n",
    "    print(f\"{desc:25s}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "We can extract pretty easily with PyTorch the probabilities the model assigns to the correct next character for each bigram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[torch.arange(len(probs)), ys]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "Our loss is, as before, the average negative log-likelihood from these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -probs[torch.arange(len(probs)), ys].log().mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "The training phase has two main steps: the forward and the backward pass.\n",
    "\n",
    "**Forward Pass**:\n",
    "we feed input data through the network to obtain predictions (in this case, the probabilities for the next character).\n",
    "We then compute a loss value, which measures how far off the network’s predictions are from the actual answers.\n",
    "Here, the loss is the average negative log likelihood of the correct next character across all inputs.\n",
    "\n",
    "**Backward Pass**:\n",
    "The backward pass is where learning happens.\n",
    "Using the computed loss, PyTorch automatically calculates how much each parameter (the weights in `W`) contributed to the error.\n",
    "This is done via a process called **backpropagation**, which computes the gradients.\n",
    "That is, it calculates the direction and amount by which each weight should be adjusted to reduce the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "We can instruct PyTorch to track all the operations we perform on tensors by setting `requires_grad=True`.\n",
    "This is done by calling `W.requires_grad_(True)`.\n",
    "This is a common pattern in PyTorch: we create a tensor, set `requires_grad=True`, and then perform operations on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "So, in summary:\n",
    "\n",
    "- Including the loss calculation in the forward pass gives us a measure of the network’s performance for each batch.\n",
    "- Calling `.backward()` on the loss triggers PyTorch to compute gradients for all parameters with `requires_grad=True`.\n",
    "- These gradients indicate how to adjust the weights to reduce the loss.\n",
    "- Updating the weights using the gradients should improve the model’s predictions in the next iteration.\n",
    "\n",
    "This is the core of the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Let's summarize all the steps and put it all together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "#### Preparing data\n",
    "\n",
    "We break down text (a sequence of strings) into sequences of adjacent characters, called n-grams.\n",
    "Here, we're creating \"bigrams\" (pairs of consecutive characters) from our dataset.\n",
    "\n",
    "For each word in our input data:\n",
    "1. We add special start and end markers (`.`)\n",
    "2. We create pairs of adjacent characters\n",
    "3. We convert each character to its numerical index using our mapping\n",
    "\n",
    "This gives us input-output pairs `(x, y)` where:\n",
    "- `x` is the index of the current character\n",
    "- `y` is the index of the next character\n",
    "\n",
    "These pairs become our training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set of all bigrams from words\n",
    "xs, ys = [], []\n",
    "\n",
    "# Process each word to extract bigrams\n",
    "for w in words:\n",
    "    # Add start/end markers to each word\n",
    "    chars = ['.'] + list(w) + ['.']\n",
    "    \n",
    "    # Create pairs of adjacent characters\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        # Convert characters to their indices\n",
    "        idx1 = stoi[ch1]  # Current character index\n",
    "        idx2 = stoi[ch2]  # Next character index\n",
    "        \n",
    "        # Add to our training examples\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "# Print dataset size\n",
    "print(f'Number of training examples: {xs.nelement()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### Initializing the neural network\n",
    "\n",
    "Our model is a simple, single-layer, no-bias neural network represented by a weight matrix `W`.\n",
    "\n",
    "- The weight matrix has dimensions $27 \\times 27$ (input size × output size)\n",
    "- Each column represents the weights for predicting one of the 27 possible next characters\n",
    "- We set `requires_grad=True` to enable automatic gradient computation during training (this is essential for backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "A this step, we can optionally tell PyTorch to use a GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (CPU or GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network's weight matrix\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)  # Fixed seed for reproducibility\n",
    "\n",
    "# Create weight matrix (27×27) with trainable parameters\n",
    "W = torch.randn((27, 27), device=device, generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "#### Training the neural network\n",
    "\n",
    "We now train our model using gradient descent.\n",
    "For each training epoch, we:\n",
    "\n",
    "1. **Forward Pass:** Feed the input data through the network to get predictions\n",
    "   - Convert inputs to one-hot encoding\n",
    "   - Compute logits by matrix multiplication with weights\n",
    "   - Apply softmax to get probability distributions\n",
    "   - Calculate the loss (negative log likelihood)\n",
    "\n",
    "2. **Backward Pass:** Compute gradients of the loss with respect to weights\n",
    "   - Reset existing gradients\n",
    "   - Backpropagate the loss\n",
    "\n",
    "3. **Update Weights:** Adjust weights to reduce the loss\n",
    "   - Simple gradient descent: `W = W - learning_rate * gradient`\n",
    "\n",
    "The loss should decrease with each epoch, indicating the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 200 epochs\n",
    "for epoch in range(200):\n",
    "    # ----- Forward Pass -----\n",
    "    # Convert input indices to one-hot vectors\n",
    "    xenc = F.one_hot(xs, num_classes=27).float().to(device)\n",
    "    \n",
    "    # Compute raw outputs (logits)\n",
    "    logits = xenc @ W\n",
    "    \n",
    "    # Apply softmax: first exponentiate\n",
    "    counts = logits.exp()\n",
    "    \n",
    "    # Then normalize to get probabilities\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    \n",
    "    # Calculate negative log likelihood loss\n",
    "    loss = -probs[torch.arange(len(probs)), ys].log().mean()\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Loss @ epoch {epoch+1}: {loss.item():.4f}')\n",
    "    \n",
    "    # ----- Backward Pass -----\n",
    "    # Reset gradients\n",
    "    W.grad = None\n",
    "    \n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # ----- Update Weights -----\n",
    "    # Simple gradient descent (learning rate = 50)\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "#### Comparison with a Bigram frequency model\n",
    "\n",
    "Our trained neural network achieves a loss of around 2.46, which is nearly identical to the explicit bigram count approach (loss of 2.45).\n",
    "This similarity isn't surprising—our simple neural network is essentially learning to mimic the count-to-distribution relationship of the bigram model.\n",
    "\n",
    "**The key difference is in flexibility and scalability.**\n",
    "\n",
    "The core workflow of the neural network will remain consistent as we build more complex models:\n",
    "\n",
    "1. Initialize weights\n",
    "2. Calculate activations \n",
    "3. Convert to probabilities\n",
    "4. Optimize weights based on loss\n",
    "\n",
    "While our current neural network doesn't outperform the simpler bigram approach, its architecture allows for natural extension to more complex patterns.\n",
    "As we add layers and consider longer sequences of characters, the neural network framework will scale elegantly.\n",
    "\n",
    "Consider the fundamental scaling challenge: with a bigram model looking at just the previous character, we need to store $27^2 = 729$ probabilities (one for each possible character pair).\n",
    "If we wanted to consider the previous 10 characters to make better predictions:\n",
    "\n",
    "- A traditional n-gram approach would require storing $27^{10} \\approx 205$ trillion different probability values—completely impractical in terms of memory and impossible to train with limited data.\n",
    "\n",
    "- In contrast, a neural network can learn to **compress this information efficiently**.\n",
    "By adding more neurons and layers, we can capture complex patterns without an exponential explosion in parameters.\n",
    "A network might need only thousands or millions of parameters to effectively model these dependencies, not trillions.\n",
    "\n",
    "This is why neural networks excel at language modeling tasks where context beyond just the previous character is crucial.\n",
    "\n",
    "> **Conclusion:** \n",
    ">\n",
    ">The bigram approach quickly hits scaling limitations, while neural networks excel precisely where the bigram model fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "#### Smoothing applied to a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "Remember that with the Bigram Model we used a smoothing technique to avoid zero probabilities.\n",
    "This was basically adding a small constant to all counts (`+1` in our case).\n",
    "The bigger this constant, the more smoothed the distribution will be.\n",
    "\n",
    "This is a simple example of a **regularization** technique, which can also be applied to our neural network approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "How can we do this for a neural network?\n",
    "\n",
    "1. We add a penalty term to the loss function based on the weights\n",
    "2. This penalty pushes weights toward zero\n",
    "3. Smaller weights result in smoother, more uniform output distributions\n",
    "4. The strength of regularization is controlled by a tunable **hyperparameter**\n",
    "\n",
    "> **Note:** an hyperparameter is a parameter that is not learned from the data, but is set by the user.\n",
    "\n",
    "Think of regularization as a gentle force pulling weights toward zero.\n",
    "Without it, the model might learn to perfectly match the training data but fail to generalize.\n",
    "With regularization, we sacrifice some training accuracy for better generalization.\n",
    "\n",
    "Here we will use **L2 regularization** (also called weight decay), which adds the mean of squared weights to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regularization term (mean of squared weights)\n",
    "# This will be added to the loss function to penalize large weights\n",
    "print(\"Regularization term (L2 norm of weights):\")\n",
    "reg_term = (W**2).mean().item()\n",
    "print(f\"{reg_term:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 200 epochs with regularization\n",
    "lambda_reg = 0.01  # Regularization strength\n",
    "\n",
    "for epoch in range(200):\n",
    "    # ----- Forward Pass -----\n",
    "    # Convert input indices to one-hot vectors\n",
    "    xenc = F.one_hot(xs, num_classes=27).float().to(device)\n",
    "    \n",
    "    # Compute raw outputs (logits)\n",
    "    logits = xenc @ W\n",
    "    \n",
    "    # Apply softmax: first exponentiate\n",
    "    counts = logits.exp()\n",
    "    \n",
    "    # Then normalize to get probabilities\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    \n",
    "    # Calculate negative log likelihood loss + regularization term\n",
    "    # Main loss: how well we predict the next character\n",
    "    nll_loss = -probs[torch.arange(len(probs)), ys].log().mean()\n",
    "    # Regularization loss: penalty for large weights\n",
    "    reg_loss = lambda_reg * (W**2).mean()\n",
    "    # Combined loss\n",
    "    loss = nll_loss + reg_loss\n",
    "    \n",
    "    print(f'Loss @ epoch {epoch+1}: {loss.item():.4f}')\n",
    "    \n",
    "    # ----- Backward Pass -----\n",
    "    # Reset gradients\n",
    "    W.grad = None\n",
    "    \n",
    "    # Compute gradients (including regularization effect)\n",
    "    loss.backward()\n",
    "    \n",
    "    # ----- Update Weights -----\n",
    "    # Simple gradient descent (learning rate = 50)\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "#### Sampling from our trained model\n",
    "\n",
    "After training, we can use our model to generate new character sequences.\n",
    "\n",
    "For each position:\n",
    "1. We start with the \".\" character (index 0)\n",
    "2. The model predicts probabilities for the next character\n",
    "3. We sample from this probability distribution\n",
    "4. We continue until we generate another \".\" (end marker)\n",
    "\n",
    "This process is similar to what we did with the bigram model, but now using our neural network for predictions.\n",
    "\n",
    "Incidentally, this is the same process that drives the generation of text by large language models like ChatGPT, although we are nowhere near the sophistication of ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample new names from the trained model\n",
    "g = torch.Generator(device=device).manual_seed(2147483642)  # Fixed seed for reproducibility\n",
    "\n",
    "print(\"Generated names:\")\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0  # Start with '.' character (index 0)\n",
    "    \n",
    "    while True:\n",
    "        # ----- Generate next character -----\n",
    "        # Convert current character index to one-hot encoding\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float().to(device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        logits = xenc @ W  # Get logits (raw output scores)\n",
    "        counts = logits.exp()  # Convert to positive values\n",
    "        p = counts / counts.sum(1, keepdims=True)  # Normalize to probabilities\n",
    "        \n",
    "        # Sample from the probability distribution\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        \n",
    "        # Add the sampled character to our output\n",
    "        out.append(itos[ix])\n",
    "        \n",
    "        # Stop when we generate an end marker\n",
    "        if ix == 0:\n",
    "            break\n",
    "            \n",
    "    print(f\"{i+1}. {''.join(out)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We've successfully implemented a neural network that performs on par with the bigram model approach.\n",
    "While both achieve similar results at this stage, they differ fundamentally in their approach:\n",
    "\n",
    "- The bigram model explicitly counts and stores character transitions\n",
    "- Our neural network learns weights that implicitly represent these same patterns\n",
    "\n",
    "Looking at our generated names `(\"ryoei\", \"telon\", \"e\", \"mfoman\", \"ylx\")`, we can see that the results are still quite rudimentary.\n",
    "Some names are implausibly short, while others contain unusual character combinations that don't appear in natural language.\n",
    "\n",
    "**Why?** Our simple neural network is still limited by the same constraints as the bigram model: it only looks at the previous character when predicting the next one.\n",
    "This means it can't really capture longer patterns or dependencies in the data.\n",
    "\n",
    "The exciting part is that this is just the beginning.\n",
    "Unlike the bigram model, our neural network architecture is highly extensible:\n",
    "\n",
    "1. We can add more layers to create deeper representations\n",
    "2. We can modify the network to consider multiple previous characters\n",
    "3. We can increase the number of neurons to capture more complex patterns\n",
    "\n",
    "In the following parts, we'll explore these extensions, gradually building toward more sophisticated architectures like transformers, the foundation of modern language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this section, there are a few *proposed* exercises on going a step further than what we did.\n",
    "These exercises are **just suggestions** that will challenge what we learned.\n",
    "\n",
    "If you don't quite know where to start, that's fine: there's a lot to digest in this notebook, so feel free to come back once you're comfortable with the material shown above.\n",
    "\n",
    "You can find some *proposed* solutions in the [companion notebook](./32s_language_modeling_1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Build a Trigram model\n",
    "\n",
    "Train a trigram language model, i.e. a model that takes **two** characters as an input to predict the 3rd one.\n",
    "Feel free to use either counting or a neural net.\n",
    "Evaluate the loss.\n",
    "\n",
    "1. Did it improve over a bigram model?\n",
    "2. If yes, how much did it improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0 # Special token has position zero\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# TODO: Modify this to accomodate for trigrams\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # Two char 'sliding-window'\n",
    "        print(ch1, ch2)\n",
    "\n",
    "# TODO: Implement a Trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "### 2. Split the dataset\n",
    "\n",
    "Split the dataset randomly into $80\\%$ `train` set, $10\\%$ `dev` set, $10\\%$ `test` set.\n",
    "Train the bigram (and/or trigram) model **only** on the `train` set.\n",
    "Evaluate them on `dev` and `test` sets. \n",
    "\n",
    "What can you see?\n",
    "\n",
    "**Hint:** Have a look at [`torch.randperm`](https://docs.pytorch.org/docs/main/generated/torch.randperm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device).manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "#### Bigram model baseline\n",
    "\n",
    "Let's see first what happens when we train the Bigram model on a fraction of the data, then evaluate the model on the `dev` and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of all *bigrams*\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys) # [196113], [196113]\n",
    "num_x, num_y = xs.nelement(), ys.nelement()\n",
    "\n",
    "# TODO: Shuffle/Permute the dataset, keeping pairs in sync\n",
    "# TODO: Split the dataset into 80:10:10 for train:valid:test\n",
    "xs_bi_train, xs_bi_valid, xs_bi_test = None, None, None\n",
    "ys_bi_train, ys_bi_valid, ys_bi_test = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "#### Compare the Bigram and Trigram model\n",
    "\n",
    "If you have worked out the Trigram model, you should probably compare the two to see if there's any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO: Create set of all *trigrams*\n",
    "xs, ys = [], []\n",
    "\n",
    "# TODO: Shuffle/Permute the dataset, keeping (x,y) pairs in sync\n",
    "# TODO: Split the dataset into 80:10:10 for train:valid:test\n",
    "xs_tri_train, xs_tri_valid, xs_tri_test = None, None, None\n",
    "ys_tri_train, ys_tri_valid, ys_tri_test = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement and train a trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the trigram model on the validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "### 3. Change the loss function\n",
    "\n",
    "Instead of using the negative log-likelihood, look up and use `F.cross_entropy` instead.\n",
    "You should achieve the same result.\n",
    "Can you think of why we'd prefer to use `F.cross_entropy` instead?\n",
    "\n",
    "Here's the [documentation on `F.cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
