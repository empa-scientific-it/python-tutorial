{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# PyTorch LLM Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to give a gentle introduction to the PyTorch library, with a focus on language modeling.\n",
    "At high-level, we will build a progressively more complex **character-level language model** that can generate more text similar to the training data.\n",
    "\n",
    "The final result is not meant to be a \"production-ready\" language model, but rather a simple yet effective example of how to use PyTorch for language modeling. Along the way, we will learn the fundamental building blocks that lay the groundwork for more complex models, including the base models that powers the state-of-the-art LLMs and derived products, like our friendly and always helpful assistant ChatGPT.\n",
    "\n",
    "The final implementation will allow you to experiment with different models, starting from the most simple and basic one (a **bigram** model) to a more complex **RNN** and finally a **Transformer** model.\n",
    "In particular, we'll be following roughly some key papers:\n",
    "\n",
    "- Bigram (one character predicts the next one with a lookup table of counts)\n",
    "- Multi-Layer Perceptron (MLP): [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "- Convolutional Neural Network (CNN): [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499)\n",
    "- Recurrent Neural Network (RNN): [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    "- Long Short-Term Memory (LSTM): [Graves et al. 2014](https://arxiv.org/abs/1308.0850)\n",
    "- Gated Recurrent Unit (GRU): [Kyunghyun Cho et al. 2014](https://arxiv.org/abs/1409.1259)\n",
    "- Transformer: [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Our initial dataset is a simple list of strings that represent common names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pl.Path(\"data/names.txt\").read_text().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words), min(len(w) for w in words), max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "The information we can extract from a single name, e.g. `isabella`, is multiple:\n",
    "\n",
    "- We know that the character `i` is followed by `s`.\n",
    "- We know that, after the characters `isabell`, the following character is `a`.\n",
    "- We know that, after the characters `isabella`, the following character is `\\n` (end of string).\n",
    "\n",
    "The idea is that a single word packs multiple pieces of information regarding the statistical structure of the language it belongs to.\n",
    "And since we have about 32k words, there's quite a lot of information we can use to train even a simple language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Bigram language model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "A bigram language model is the simplest possible language model.\n",
    "Given a sequence of characters (each character is usually referred to as a **token**), the bigram language model assigns a probability to each possible next token, given the previous token.\n",
    "It's a predictor for each pair of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:3]:\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The most basic modeling of the statistical patterns embedded in our input data is predidicting the next token **by frequency**.\n",
    "We can build a simple dictionary that counts how many times a bigram (i.e., sequence of two tokens) appear in our dataset.\n",
    "\n",
    "We also need to add the *special* information about the start and end of the sequence.\n",
    "We can \"encode\" that information with two **special tokens**: `<S>` (start) and `<E>` (end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#% Student code\n",
    "from collections import defaultdict\n",
    "\n",
    "bigrams = defaultdict(int)\n",
    "\n",
    "for w in words:\n",
    "    chars = [\"<S>\"] + list(w) + [\"<E>\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        bigrams[(ch1, ch2)] += 1\n",
    "\n",
    "print(dict(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "What does an entry of the bigram dictionary look like?\n",
    "It's something like `('a', '<E>'): 6640`, which means: the bigram `('a', '<E>')` occurred 6640 times.\n",
    "That is: the letter `a` is quite likely to appear at the end of a name.\n",
    "\n",
    "We want to sort the bigrams by their count, from the most frequent to the least frequent.\n",
    "Let's see the first 10 most frequent bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(bigrams.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "A much better way to store this information in a 2D array, where the rows are going to be the 1st character and the columns are going to be the 2nd character.\n",
    "The entry at row `a` and column `b` is going to be the count of the bigram `ab`.\n",
    "\n",
    "Since we are dealing with 26 characters, plus `<S>` and `<E>`, we need a total of 28x28 = 784 entries.\n",
    "Bracketed tokens are customary in NLP to represent special tokens, but here we are only interested in knowing when a sentence starts or ends.\n",
    "We can replace `<S>` with `.` and `<E>` with `.` and have a total of 27x27 = 729 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "How should we encode the bigrams?\n",
    "Our 2D array is going to hold integers **only**, so we need a way to make this conversion.\n",
    "One way is to be a so-called **vocabulary** from our input data.\n",
    "\n",
    "A vocabulary requires two functions:\n",
    "1. `stoi`: string to integer (**encoding**)\n",
    "2. `itos`: integer to string (**decoding**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(''.join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0 # special token for end of sentence is mapped to 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#% Student code\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    i = stoi[ch1]\n",
    "    j = stoi[ch2]\n",
    "    N[i, j] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Let's create a nice visualization of our bigram frequency table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "norm = mpl.colors.Normalize(vmin=N.min(), vmax=N.max())\n",
    "im = ax.imshow(N, cmap='Blues')\n",
    "\n",
    "for i in range(N.shape[0]):\n",
    "    for j in range(N.shape[1]):\n",
    "        val = norm(N[i,j])\n",
    "        text_color = 'white' if val > 0.5 else 'black'\n",
    "        # character\n",
    "        ax.text(j, i, itos[i]+itos[j],\n",
    "                ha=\"center\", va=\"bottom\",\n",
    "                color=text_color, fontweight='bold')\n",
    "        # count\n",
    "        ax.text(j, i, int(N[i,j]),\n",
    "                ha=\"center\", va=\"top\",\n",
    "                color=text_color)\n",
    "\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Our bigram model is essentially an iterative sampling from a probability distribution that describes how frequent each bigram is in the dataset.\n",
    "\n",
    "We already have the frequency table, so we need to built a probability distribution and a sampling mechanism.\n",
    "Let's do it for the first row, which represents the frequency of each bigram starting with the character '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = N[0].float()\n",
    "proba /= proba.sum()\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "PyTorch provides us with a method to sample from a [**Multinomial distribution**](https://docs.pytorch.org/docs/main/distributions.html#multinomial).\n",
    "A Multinomial distribution is a generalization of a [**Binomial distribution**](https://en.wikipedia.org/wiki/Binomial_distribution), where we sample from a distribution with more than two outcomes.\n",
    "\n",
    "To enforce predictability, we can initialize a random number generator with a fixed seed.\n",
    "Also, we need to allow sampling **with replacement**, so that we can sample the same token multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(proba, num_samples=1, replacement=True, generator=gen).item()\n",
    "print(itos[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We can of course sample as many tokens as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(proba, num_samples=100, replacement=True, generator=gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "You might have understood how the process goes: after we extract a given bigram, we need to lookup the most likely bigram that starts with the second character of the first bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for _ in range(10):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "\n",
    "  while True:\n",
    "    # Compute the probabilities\n",
    "    p = N[ix].float()\n",
    "    p /= p.sum()\n",
    "    \n",
    "    # Sample the next character\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    \n",
    "    # Add the character to the output\n",
    "    out.append(itos[ix])\n",
    "    \n",
    "    # Stop if we reach the end of the text\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The result is not very good.\n",
    "The core problem is that a bigram model looks only the the frequency of a pair of tokens, but it has zero information of what's most likely to come before or after those two tokens.\n",
    "You can imagine that the obvious next step is a **trigram** model, which looks at the frequency of a triplet of tokens.\n",
    "\n",
    "Let's now improve a bit our code: the first thing is to compute **all** the probabilities once, and then sample from them.\n",
    "PyTorch tensors support **vectorized** operations, which means that we can perform operations on entire tensors at once, without having to loop through them.\n",
    "\n",
    "Each row of our 2D matrix contains the counts of how many times the token with that row index is followed by all the other tokens, whose indexes run along the columns.\n",
    "\n",
    "$$\n",
    "P_{ij}= \\frac{N_{ij}}{\\displaystyle\\sum_{k} N_{i k}}\n",
    "$$\n",
    "\n",
    "For each pair $(i,j)$:\n",
    "- The numerator $N_{ij}$ is the count of the number of times token `j` follows token `i`.\n",
    "- The denominator $\\sum_{k} N_{i k}$ is the total number of times *any* character follows `i`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "In Python\n",
    "\n",
    "```python\n",
    "P = N.float()\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Here `dim=1` tells PyTorch to sum over the columns (the second index), while `keepdim=True` tells it to keep the first dimension (the first index) as a singleton (a `1`) dimension.\n",
    "Without `keepdim=True`, the result would have shape `(27,)`, and performing the division would produce the wrong result because of how [brodcasting](https://pytorch.org/docs/stable/notes/broadcasting.html) works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "P = N.float()\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "\n",
    "for _ in range(10):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "\n",
    "  while True:\n",
    "    # Get the probabilities\n",
    "    p = P[ix]\n",
    "    \n",
    "    # Sample the next character\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    \n",
    "    # Add the character to the output\n",
    "    out.append(itos[ix])\n",
    "    \n",
    "    # Stop if we reach the end of the text\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
