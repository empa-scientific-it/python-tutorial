{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Language Modeling With PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to give a gentle introduction to the PyTorch library, with a focus on language modeling.\n",
    "At high-level, we will build a progressively more complex **character-level language model** that can generate more text similar to the training data.\n",
    "\n",
    "The final result is not meant to be a \"production-ready\" language model, but rather a simple yet effective example of how to use PyTorch for language modeling. Along the way, we will learn the fundamental building blocks that lay the groundwork for more complex models, including the base models that powers the state-of-the-art LLMs and derived products, like our friendly and always helpful assistant ChatGPT.\n",
    "\n",
    "The final implementation will allow you to experiment with different models, starting from the most simple and basic one (a **bigram** model) to a more complex **RNN** and finally a **Transformer** model.\n",
    "In particular, we'll be following roughly some key papers:\n",
    "\n",
    "- Bigram (one character predicts the next one with a lookup table of counts)\n",
    "- Multi-Layer Perceptron (MLP): [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "- Convolutional Neural Network (CNN): [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499)\n",
    "- Recurrent Neural Network (RNN): [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    "- Long Short-Term Memory (LSTM): [Graves et al. 2014](https://arxiv.org/abs/1308.0850)\n",
    "- Gated Recurrent Unit (GRU): [Kyunghyun Cho et al. 2014](https://arxiv.org/abs/1409.1259)\n",
    "- Transformer: [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Our initial dataset is a simple list of strings that represent common names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pl.Path(\"data/names.txt\").read_text().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words), min(len(w) for w in words), max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "The information we can extract from a single name, e.g. `isabella`, is multiple:\n",
    "\n",
    "- We know that the character `i` is followed by `s`.\n",
    "- We know that, after the characters `isabell`, the following character is `a`.\n",
    "- We know that, after the characters `isabella`, the following character is `\\n` (end of string).\n",
    "\n",
    "The idea is that a single word packs multiple pieces of information regarding the statistical structure of the language it belongs to.\n",
    "And since we have about 32k words, there's quite a lot of information we can use to train even a simple language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Bigram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "A bigram language model is the simplest possible language model.\n",
    "Given a sequence of characters (each character is usually referred to as a **token**), the bigram language model assigns a probability to each possible next token, given the previous token.\n",
    "It's a predictor for each pair of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:3]:\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The most basic modeling of the statistical patterns embedded in our input data is predidicting the next token **by frequency**.\n",
    "We can build a simple dictionary that counts how many times a bigram (i.e., sequence of two tokens) appear in our dataset.\n",
    "\n",
    "We also need to add the *special* information about the start and end of the sequence.\n",
    "We can \"encode\" that information with two **special tokens**: `<S>` (start) and `<E>` (end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#% Student code\n",
    "from collections import defaultdict\n",
    "\n",
    "bigrams = defaultdict(int)\n",
    "\n",
    "for w in words:\n",
    "    chars = [\"<S>\"] + list(w) + [\"<E>\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        bigrams[(ch1, ch2)] += 1\n",
    "\n",
    "print(dict(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "What does an entry of the bigram dictionary look like?\n",
    "It's something like `('a', '<E>'): 6640`, which means: the bigram `('a', '<E>')` occurred 6640 times.\n",
    "That is: the letter `a` is quite likely to appear at the end of a name.\n",
    "\n",
    "We want to sort the bigrams by their count, from the most frequent to the least frequent.\n",
    "Let's see the first 10 most frequent bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(bigrams.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "A much better way to store this information in a 2D array, where the rows are going to be the 1st character and the columns are going to be the 2nd character.\n",
    "The entry at row `a` and column `b` is going to be the count of the bigram `ab`.\n",
    "\n",
    "Since we are dealing with 26 characters, plus `<S>` and `<E>`, we need a total of 28x28 = 784 entries.\n",
    "Bracketed tokens are customary in NLP to represent special tokens, but here we are only interested in knowing when a sentence starts or ends.\n",
    "\n",
    "But there's also a problem with keeping two special tokens for the start and end of a sentence: we can't have `('<S>', '<S>')` or `('<E>', '<E>')`, or other combinations like `('a', '<S>')` or `('<E>', 'b')`.\n",
    "These would be invalid bigrams.\n",
    "To solve this, we can replace `<S>` with `.` and `<E>` with `.` and have a total of 27x27 = 729 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "How should we encode the bigrams?\n",
    "Our 2D array is going to hold integers **only**, so we need a way to make this conversion.\n",
    "One way is to be a so-called **vocabulary** from our input data.\n",
    "\n",
    "A vocabulary requires two functions:\n",
    "1. `stoi`: string to integer (**encoding**)\n",
    "2. `itos`: integer to string (**decoding**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(''.join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0 # special token for end of sentence is mapped to 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#% Student code\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    i = stoi[ch1]\n",
    "    j = stoi[ch2]\n",
    "    N[i, j] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Let's create a nice visualization of our bigram frequency table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "norm = mpl.colors.Normalize(vmin=N.min(), vmax=N.max())\n",
    "im = ax.imshow(N, cmap='Blues')\n",
    "\n",
    "for i in range(N.shape[0]):\n",
    "    for j in range(N.shape[1]):\n",
    "        val = norm(N[i,j])\n",
    "        text_color = 'white' if val > 0.5 else 'black'\n",
    "        # character\n",
    "        ax.text(j, i, itos[i]+itos[j],\n",
    "                ha=\"center\", va=\"bottom\",\n",
    "                color=text_color, fontweight='bold')\n",
    "        # count\n",
    "        ax.text(j, i, int(N[i,j]),\n",
    "                ha=\"center\", va=\"top\",\n",
    "                color=text_color)\n",
    "\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Our bigram model is essentially an iterative sampling from a probability distribution that describes how frequent each bigram is in the dataset.\n",
    "\n",
    "We already have the frequency table, so we need to built a probability distribution and a sampling mechanism.\n",
    "Let's do it for the first row, which represents the frequency of each bigram starting with the character '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = N[0].float()\n",
    "proba /= proba.sum()\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "PyTorch provides us with a method to sample from a [**Multinomial distribution**](https://docs.pytorch.org/docs/main/distributions.html#multinomial).\n",
    "A Multinomial distribution is a generalization of a [**Binomial distribution**](https://en.wikipedia.org/wiki/Binomial_distribution), where we sample from a distribution with more than two outcomes.\n",
    "\n",
    "To enforce predictability, we can initialize a random number generator with a fixed seed.\n",
    "Also, we need to allow sampling **with replacement**, so that we can sample the same token multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(proba, num_samples=1, replacement=True, generator=gen).item()\n",
    "print(itos[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We can of course sample as many tokens as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(proba, num_samples=100, replacement=True, generator=gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "You might have understood how the process goes: after we extract a given bigram, we need to lookup the most likely bigram that starts with the second character of the first bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "NUM_WORDS = 20\n",
    "\n",
    "for _ in range(NUM_WORDS):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "\n",
    "  while True:\n",
    "    # Compute the probabilities\n",
    "    p = N[ix].float()\n",
    "    p /= p.sum()\n",
    "    \n",
    "    # Sample the next character\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    \n",
    "    # Add the character to the output\n",
    "    out.append(itos[ix])\n",
    "    \n",
    "    # Stop if we reach the end of the text\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The results are quite terrible, although they're reasonable given the simplicity of the model and the patterns we're trying to capture.\n",
    "\n",
    "The core problem is that a bigram model looks only the the frequency of a pair of tokens, but it has zero information of what's most likely to come before or after those two tokens.\n",
    "You can imagine that the obvious next step is a **trigram** model, which looks at the frequency of a triplet of tokens.\n",
    "\n",
    "Let's now improve a bit our code: the first thing is to compute **all** the probabilities once, and then sample from them.\n",
    "PyTorch tensors support **vectorized** operations, which means that we can perform operations on entire tensors at once, without having to loop through them.\n",
    "\n",
    "Each row of our 2D matrix contains the counts of how many times the token with that row index is followed by all the other tokens, whose indexes run along the columns.\n",
    "\n",
    "$$\n",
    "P_{ij}= \\frac{N_{ij}}{\\displaystyle\\sum_{k} N_{i k}}\n",
    "$$\n",
    "\n",
    "For each pair $(i,j)$:\n",
    "- The numerator $N_{ij}$ is the count of the number of times token `j` follows token `i`.\n",
    "- The denominator $\\sum_{k} N_{i k}$ is the total number of times *any* character follows `i`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "In Python\n",
    "\n",
    "```python\n",
    "P = N.float()\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Here `dim=1` tells PyTorch to sum over the columns (the second index), while `keepdim=True` tells it to keep the first dimension (the first index) as a singleton (a `1`) dimension.\n",
    "Without `keepdim=True`, the result would have shape `(27,)`, and performing the division would produce the wrong result because of how [brodcasting](https://pytorch.org/docs/stable/notes/broadcasting.html) works.\n",
    "\n",
    "> Try to experiment with the `keepdim` parameter and see what happens if you remove it.\n",
    "> Can you explain why the predictions become complete garbage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "P = N.float()\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "\n",
    "for _ in range(10):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "\n",
    "  while True:\n",
    "    # Get the probabilities\n",
    "    p = P[ix]\n",
    "    \n",
    "    # Sample the next character\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    \n",
    "    # Add the character to the output\n",
    "    out.append(itos[ix])\n",
    "    \n",
    "    # Stop if we reach the end of the text\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Evaluating the quality of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "We have built a bigram language model by counting letter combination frequencies, then normalizing and sampling with that probability base.\n",
    "\n",
    "We trained the model, we sampled from the model (iteratively, character-wise). But its still bad at coming up with names.\n",
    "\n",
    "But how bad? We know that the model's \"knowledge\" is represented by `P`, but how can we boil down the model's quality in one value?\n",
    "\n",
    "First, let's look at the bigrams we created from the dataset: the bigrams to `emma` are `.e, em, mm, ma, a.`.\n",
    "**What probability does the model assign to each of those bigrams?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # Neat way for two char 'sliding-window'\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        print(f'{ch1}{ch2}: {prob:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Anything above or below $\\frac{1}{27} \\approx 3.7\\%$ means we deviate from the mean, that is, a completely uniform distribution of bigrams. \n",
    "And that means we learned something from the bigram statistics.\n",
    "\n",
    "How can we summarize these probabilities into a quality indicating measurement?\n",
    "We may compute the product of all probabilities — a number called the **likelihood**.\n",
    "But since all these probabilities are small numbers, the product is also a small number, and it is hard to compare likelihoods.\n",
    "Solution: *The log-likelihood, the **sum** of $\\log(P)$ over all the individual token probabilities* ($\\log$ is applied for convenience).\n",
    "\n",
    "> The higher the log-likelihood, the better the model, because the more capable it is of predicting the next character in a sequence from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "log_likelihood = 0.0\n",
    "n = 0  # character pair count\n",
    "\n",
    "for word in words:\n",
    "    # Add start/end tokens and convert to character list\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    \n",
    "    # Calculate log probabilities in a more compact way\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        prob = P[stoi[ch1], stoi[ch2]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        n += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')  # Negative log likelihood\n",
    "print(f'Average NLL: {nll/n:.4f}')  # More descriptive output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "We calculated a negative log-likelihood, because this follows the convention of setting the goal to minimize the **loss function**, the function that drives the optimization (i.e., training) process.\n",
    "The lower the loss/negative log-likelihood, the better the model.\n",
    "\n",
    "We got $2.45$ for the model. The lower, the better.\n",
    "We need to find the parameters that reduce this value.\n",
    "\n",
    "**Goal:** Maximize likelihood of the trained data w. r. t. model parameters in `P`\n",
    "- This is equivalent to maximizing the log-likelihood (as $\\log$ is monotonic)\n",
    "- This is equivalent to minimizing the *negative* log-likelihood\n",
    "- And this is equivalent to minimizing the average negative log-likelihood (the quality-measurement, as shown by $2.45$ above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "There's an immediate problem, though: if we have a word containing a bigram that **never** appears in our training data, the model will assign a probability of $0$ to it, which will make the log-likelihood $-\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "log_likelihood = 0.0\n",
    "n = 0  # character pair count\n",
    "\n",
    "for word in [\"edobq\"]:\n",
    "    # Add start/end tokens and convert to character list\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    \n",
    "    # Calculate log probabilities in a more compact way\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        prob = P[stoi[ch1], stoi[ch2]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        n += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')  # Negative log likelihood\n",
    "print(f'Average NLL: {nll/n:.4f}')  # More descriptive output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "A negative infinite log-likelihood is definitely not good because our optimizer will never find a \"stable\" solution.\n",
    "\n",
    "One simple fix is to assign a small but non-zero probability to every bigram: this is called **model smoothing**.\n",
    "The easiest way is to ensure that no bigram *never* appears: we can achieve this by adding a constant to our 2D matrix `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS = (N + 1).float()  # The higher the number, the more smoothing we apply\n",
    "PS /= PS.sum(dim=1, keepdim=True)\n",
    "\n",
    "# Initialize variables\n",
    "log_likelihood = 0.0\n",
    "n = 0  # character pair count\n",
    "\n",
    "for word in [\"edobq\"]:\n",
    "    # Add start/end tokens and convert to character list\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    \n",
    "    # Calculate log probabilities in a more compact way\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        prob = PS[stoi[ch1], stoi[ch2]]  # Use the smoothed probabilities\n",
    "        log_likelihood += torch.log(prob)\n",
    "        n += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')  # Negative log likelihood\n",
    "print(f'Average NLL: {nll/n:.4f}')  # More descriptive output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## A neural network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "We will cast the problem of character estimation into the framework of neural networks.\n",
    "The problem remains the same, the approach changes, and the outcome should look similar.\n",
    "\n",
    "Our neural network **receives a single character** and **outputs the probability distribution over the next possible characters** ($27$ in this case).\n",
    "\n",
    "It's going to make guesses on the most likely character to follow.\n",
    "We can still measure the performance through the *same* loss function, the negative log-likelihood.\n",
    "\n",
    "From the training data, we also know the character that actually comes next in each training example.\n",
    "We'll use this information to fine-tune (i.e., train or update the parameters of) the neural network to make better guesses: this is a textbook example of **supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### The training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training set of all bigrams\n",
    "xs, ys = [], [] # Input and output character indices\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "# Convert lists to tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f'For character #{i} \"{itos[xs[i].item()]}\" in xs, we expect the model to predict \"{itos[ys[i].item()]}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
