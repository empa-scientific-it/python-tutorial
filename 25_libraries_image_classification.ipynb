{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Image Classification Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "  - [Image Classification Notebook](#Image-Classification-Notebook)\n",
    "    - [References](#References)\n",
    "    - [Libraries](#Libraries)\n",
    "    - [Introduction](#Introduction)\n",
    "    - [Classes](#Classes)\n",
    "    - [Functions](#Functions)\n",
    "    - [Dataset](#Dataset)\n",
    "      - [Load data](#Load-data)\n",
    "    - [Explore image processing](#Explore-image-processing)\n",
    "      - [Geometric transformation](#Geometric-transformation)\n",
    "        - [Scaling](#Scaling)\n",
    "        - [Cropping](#Cropping)\n",
    "        - [Vertical flip](#Vertical-flip)\n",
    "        - [Horizontal flip](#Horizontal-flip)\n",
    "        - [Rotation](#Rotation)\n",
    "      - [Image filtering](#Image-filtering)\n",
    "        - [Average filter](#Average-filter)\n",
    "        - [Median filter](#Median-filter)\n",
    "        - [Gaussian filter](#Gaussian-filter)\n",
    "      - [Photometric transformation](#Photometric-transformation)\n",
    "        - [Adjust brightness](#Adjust-brightness)\n",
    "        - [Adjust contrast](#Adjust-contrast)\n",
    "        - [Adjust saturation](#Adjust-saturation)\n",
    "    - [CNN model development](#CNN-model-development)\n",
    "      - [Data preprocessing](#Data-preprocessing)\n",
    "        - [Data Augmentation](#Data-augmentation)\n",
    "        - [Train, validation, and test sets](#Train-validation-and-test-sets)\n",
    "        - [PyTorch datasets](#Pytorch-datasets)\n",
    "        - [PyTorch dataloaders](#Pytorch-dataloaders)\n",
    "      - [Model training](#Model-training)\n",
    "        - [Training hyperparameters](#Training-hyperparameters)\n",
    "        - [Initialise model architecture](#Initialise-model-architecture)\n",
    "        - [Optimiser](#Optimiser)\n",
    "        - [Train model](#Train-model)\n",
    "        - [Loss function](#Loss-function)\n",
    "      - [Learning curves](#Learning-curves)\n",
    "      - [Model testing](#Model-testing)\n",
    "      - [Explore results](#Explore-results)\n",
    "        - [Compute average accuracy](#Compute-average-accuracy)\n",
    "        - [Compute confusion matrix](#Compute-confusion-matrix)\n",
    "      - [Explain image prediction](#Explain-image-prediction)\n",
    "        - [Load data batch](#Load-data-batch)\n",
    "        - [Compute GradCAM heatmap](#Compute-GradCAM-heatmap)\n",
    "        - [Visualise GradCAM heatmap with the image](#Visualise-GradCAM-heatmap-with-the-image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "- NumPy\n",
    "- Matplotlib\n",
    "- Scikit-learn\n",
    "- OpenCV-Python\n",
    "- PyTorch\n",
    "- Albumentations\n",
    "- PyTorch GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important modules\n",
    "import pytest\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Here are some additional references to guide you while self-learning:\n",
    "- Official documentation for [openCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)\n",
    "- Official documentation for [PIL library](https://pillow.readthedocs.io/en/stable/)\n",
    "- Official documentation for [PyTorch](https://pytorch.org/)\n",
    "- Official documentation for [Albumentations](https://albumentations.ai/)\n",
    "- Official documentation for [PyTorch GradCAM](https://jacobgil.github.io/pytorch-gradcam-book/introduction.html)\n",
    "- [A tutorial from Microsoft to compute image classification using PyTorch](https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-train-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Image Classification is a foundational task in the field of computer vision and machine learning. This notebook aims to provide practical experience in image processing and in building and evaluating image classification models. It begins by demonstrating how to load and preprocess image data using Matplotlib and OpenCV-Python. Then, it shows how to build a basic image classification pipeline based on Convolutional Neural Networks (CNNs) using PyTorch, Albumentations, and Scikit-learn. Next, it covers how to evaluate model performance using Scikit-learn and NumPy, and finally, it introduces model explainability using Grad-CAM.\n",
    "\n",
    "The goal of this notebook is not to teach the underlying algorithms and procedures used in this field, but rather to give the user an idea of what can be done with these Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Classes\n",
    "\n",
    "The following three classes are essential for improving modularity and readability.\n",
    "\n",
    "- **ImageDataset** is used to load images along with their labels and to perform image augmentation.\n",
    "- **ImageClassifier** is responsible for building the image classification model, which in this case is based on Convolutional Neural Networks (CNNs).\n",
    "- **Trainer** handles the training and evaluation processes using batches of data.\n",
    "\n",
    "By organizing the code in this way, we simplify debugging and future extensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    " \n",
    "        # Ensure the image is in the shape (H, W, C) for Albumentations\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        \n",
    "        # Apply Albumentations transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_classes = 1):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        \n",
    "        # Best model weights\n",
    "        self.best_model_weights = None\n",
    "        \n",
    "        # Convolutional block layers\n",
    "        self.feature_maps = 64\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels, self.feature_maps, kernel_size = 3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.bn1 = nn.BatchNorm2d(self.feature_maps)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(self.feature_maps, self.feature_maps * 2, kernel_size = 3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.bn2 = nn.BatchNorm2d(self.feature_maps * 2)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(self.feature_maps * 2, self.feature_maps * 4, kernel_size = 3)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.bn3 = nn.BatchNorm2d(self.feature_maps * 4)\n",
    "        \n",
    "        # Rectified linear unit (activation function)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Flatten layer that produces the features vector\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        \n",
    "        # Dropout layer is responsible for introducing regularisation into training process\n",
    "        self.dropout = nn.Dropout(p = 0.3)\n",
    "        \n",
    "        # Number of output classes\n",
    "        self.out_classes = out_classes\n",
    "        \n",
    "        # Classification layer \n",
    "        self.fc = nn.Linear(1024, self.out_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        # Convolutional block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        # Convolutional block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def fit(self, epochs, train_dataloader, val_dataloader, optimizer, criterion, device, early_stopping_limit = 0):\n",
    "        early_stopping_count = 0\n",
    "        best_val_loss = 9999\n",
    "        best_epoch = 0\n",
    "        \n",
    "        # Training log file\n",
    "        log_filename = \"training_log.txt\"\n",
    "        with open(log_filename, \"w\") as log_file:\n",
    "            log_file.write(\"Epoch,Train Loss,Val Loss,Best Val Loss,Best Epoch\\n\")\n",
    "\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            train_loss = 0.0\n",
    "            train_samples_count = 0.0\n",
    "            for i, data in enumerate(train_dataloader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.long().to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward step\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                # Backward step\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Weight optimisation\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                train_samples_count += 1\n",
    "                \n",
    "            # Validation mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            val_loss = 0.0\n",
    "            val_samples_count = 0.0\n",
    "            for i, data in enumerate(val_dataloader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.long().to(device)\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_samples_count += 1\n",
    "            \n",
    "            train_loss /= train_samples_count\n",
    "            val_loss /= val_samples_count\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            early_stopping_count += 1\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_epoch = epoch\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_count = 0\n",
    "                self.model.best_model_weights = self.model.state_dict()\n",
    "            \n",
    "                \n",
    "            print(f'Epoch: {epoch}, Loss: {train_loss}, Val Loss: {val_loss}. The best val loss is {best_val_loss} in epoch {best_epoch}.')\n",
    "            \n",
    "            with open(log_filename, \"a\") as log_file:\n",
    "                log_file.write(f\"{epoch},{train_loss},{val_loss},{best_val_loss},{best_epoch}\\n\")\n",
    "            \n",
    "            if early_stopping_count == early_stopping_limit and early_stopping_limit > 0:\n",
    "                break\n",
    "    \n",
    "    def predict(self, test_dataloader, device):\n",
    "        # Load best weights\n",
    "        if self.model.best_model_weights:\n",
    "            self.model.load_state_dict(self.model.best_model_weights)\n",
    "\n",
    "        # Test mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        original_images = []\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            outputs = self.model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            images = images.cpu().detach().numpy()\n",
    "            labels = labels.numpy()\n",
    "            predicted = predicted.cpu().detach().numpy()\n",
    "            \n",
    "            original_images.append(images)\n",
    "            true_labels.append(labels)\n",
    "            predicted_labels.append(predicted)\n",
    "        \n",
    "        original_images = np.concatenate(original_images)\n",
    "        true_labels = np.concatenate(true_labels)\n",
    "        predicted_labels = np.concatenate(predicted_labels)\n",
    "        \n",
    "        return original_images, true_labels, predicted_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The following three functions are going to be used throughout the notebook. They comprise the loading of binary files using Pickle (**load_pickle_file**), single image plotting (**plot_image**), and multiple image plotting (**plot_multiple_images**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_file(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def plot_image(img, figsize = (2,3)):\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def plot_multiple_images(*images_titles, figsize = (2, 3)):\n",
    "    num_images = len(images_titles)\n",
    "    fig, axs = plt.subplots(1, num_images, figsize = figsize)\n",
    "    for i in range(num_images):\n",
    "        axs[i].imshow(images_titles[i][0])\n",
    "        axs[i].set_title(images_titles[i][1])\n",
    "        axs[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this section, we load the CIFAR-10 dataset, which consists of 60,000 32x32 color images across 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 test images. It was already processed and it is ready to use after loading the binary files *train_set.pkl* and *test_set.pkl*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Training and test sets are loaded using Pickle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets filepaths\n",
    "dataset_folder = os.path.join(\"CIFAR10\")\n",
    "train_set_file = os.path.join(dataset_folder, \"train_set.pkl\")\n",
    "test_set_file = os.path.join(dataset_folder, \"test_set.pkl\")\n",
    "\n",
    "# Load sets\n",
    "train_set = load_pickle_file(train_set_file)\n",
    "test_set = load_pickle_file(test_set_file)\n",
    "\n",
    "# CIFAR10 classes\n",
    "CIFAR_10_CLASSES = [\n",
    "    \"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\",\n",
    "    \"Dog\", \"Frog\", \"Horse\",\"Ship\",\"Truck\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Explore image processing\n",
    "\n",
    "Image processing is fundamental to computer vision, forming the basis for interpreting and analyzing visual information. By applying techniques such as resizing, filtering, color adjustments, and data augmentation, image processing enhances input quality, minimizes noise, and corrects distortions. These methods can also simulate real-world variability, helping models generalize better. In this notebook, we explore three categories of image transformations: geometric transformations, image filtering, and photometric transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select image\n",
    "image = train_set[0][9]\n",
    "\n",
    "# Convert image from (C, H, W) to (H, W, C)\n",
    "image = np.transpose(image, (1,2,0))\n",
    "\n",
    "# Plot image\n",
    "plot_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Geometric transformation\n",
    "\n",
    "Geometric transformations alter the spatial structure of the image while preserving its semantic content. They help the model become invariant to different orientations and scales:\n",
    "- **Scaling**: Resizes the image to a specific size, often required to match input dimensions for image classifiers. It uses interpolation to obtain the new pixel-values.\n",
    "- **Cropping**: Extracts a subregion of the image; useful for focusing on important parts or adding variability.\n",
    "- **Horizontal and vertical flip**: Flips the image along the x-axis or y-axis; helps the model learn symmetry.\n",
    "- **Rotation**: Rotates the image by a small angle to simulate different orientations of the objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tutorial.tests.testsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def solution_scale_image(img, scale_factor: float):\n",
    "    # Get the current dimensions\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    # Calculate the new dimensions\n",
    "    new_width = int(width * scale_factor)\n",
    "    new_height = int(height * scale_factor)\n",
    "    new_size = (new_width, new_height)\n",
    "\n",
    "    # Resize the image\n",
    "    return cv2.resize(img, new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale image by half\n",
    "scaled_image = solution_scale_image(image, 0.5)\n",
    "\n",
    "if scaled_image is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (scaled_image, \"Scaled\"), figsize = (4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_crop_image(img, x: int, y: int, width: int, height: int):\n",
    "    x1, x2, y1, y2 = x, x+width, y, y+height\n",
    "    return img[y:y + height, x:x + width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop image to get a 15-by-15 image starting on (x,y): (2,2)\n",
    "cropped_image = solution_crop_image(image, 2, 2, 15, 15)\n",
    "\n",
    "if cropped_image is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (cropped_image, \"Cropped\"), figsize = (4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_horizontal_flip_image(img):\n",
    "    return cv2.flip(img, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip image horizontally\n",
    "flip_image_horizontal = solution_horizontal_flip_image(image)\n",
    "\n",
    "if flip_image_horizontal is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (flip_image_horizontal, \"Horizontal Flip\"), figsize = (4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### Vertical Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_vertical_flip_image(img):\n",
    "    return cv2.flip(img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip image vertically\n",
    "flip_image_vertical = solution_vertical_flip_image(image)\n",
    "\n",
    "if flip_image_vertical is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (flip_image_vertical, \"Vertical Flip\"), figsize = (4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_rotate_image(img, angle: float):\n",
    "    (h, w) = img.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, scale=1.0)\n",
    "    \n",
    "    # Compute new bounding dimensions\n",
    "    cos = np.abs(M[0, 0])\n",
    "    sin = np.abs(M[0, 1])\n",
    "    new_w = int((h * sin) + (w * cos))\n",
    "    new_h = int((h * cos) + (w * sin))\n",
    "\n",
    "    # Adjust rotation matrix for translation\n",
    "    M[0, 2] += (new_w / 2) - center[0]\n",
    "    M[1, 2] += (new_h / 2) - center[1]\n",
    "\n",
    "    # Perform rotation with expanded canvas\n",
    "    return cv2.warpAffine(img, M, (new_w, new_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate image by 20 degrees\n",
    "rotated_image = solution_rotate_image(image, 20)\n",
    "\n",
    "if rotated_image is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (rotated_image, \"Rotated\"), figsize = (4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Image filtering\n",
    "\n",
    "Filtering helps reduce noise and enhance specific image features. These are often used as a form of preprocessing before feeding images into a model:\n",
    "- **Average filter**: Applies a smoothing effect by replacing each pixel with the average of its neighborhood.\n",
    "- **Median filter**: Reduces salt-and-pepper noise by replacing each pixel with the median of neighboring pixels.\n",
    "- **Gaussian filter**: Applies a Gaussian blur to smooth the image, often used to reduce high-frequency noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "#### Average filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_average_filter(img, kernel_size = (5, 5)):\n",
    "    return cv2.blur(img, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter image using average filter\n",
    "average_filter_image = solution_average_filter(image, (3, 3))\n",
    "\n",
    "if average_filter_image is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (average_filter_image, \"Average filter\"), figsize = (4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Median filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_median_filter(img, ksize):\n",
    "    return cv2.medianBlur(img, ksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter image using median filter\n",
    "median_filter_image = solution_median_filter(image, 3)\n",
    "\n",
    "if median_filter_image is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (median_filter_image, \"Median filter\"), figsize = (4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Gaussian filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_gaussian_filter(img, kernel_size = (5, 5), sigma = 0):\n",
    "    return cv2.GaussianBlur(img, kernel_size, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter image using Gaussian filter\n",
    "gaussian_filter_image = solution_gaussian_filter(image, (7, 7), 0)\n",
    "\n",
    "if gaussian_filter_image is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (gaussian_filter_image, \"Gaussian filter\"), figsize = (4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Photometric transformation\n",
    "\n",
    "Photometric transformations modify the color properties of an image to simulate different lighting conditions and improve model robustness to brightness and contrast changes:\n",
    "- **Brightness**: Randomly increases or decreases the brightness of the image.\n",
    "- **Contrast**: Alters the difference between light and dark regions in the image.\n",
    "- **Saturation**: Modifies the intensity of the colors in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Adjust brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_adjust_brightness(img, brightness_value):\n",
    "    return cv2.convertScaleAbs(img, beta=brightness_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brighter image (positive brightness value)\n",
    "brighter_image = solution_adjust_brightness(image, 100)\n",
    "\n",
    "# Darker image (negative brightness value)\n",
    "darker_image = solution_adjust_brightness(image, -100)\n",
    "\n",
    "if brighter_image is not None and darker_image is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (brighter_image, \"Brighter image\"), (darker_image, \"Darker image\"), figsize = (7, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Adjust contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_adjust_contrast(img, contrast_value):\n",
    "    return cv2.convertScaleAbs(img, alpha = contrast_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase contrast (Value > 1.0)\n",
    "high_contrast_image = solution_adjust_contrast(image, 2.0)\n",
    "\n",
    "# Reduce contrast (Value < 1.0)\n",
    "low_contrast_image = solution_adjust_contrast(image, 0.5)\n",
    "\n",
    "if high_contrast_image is not None and low_contrast_image is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (high_contrast_image, \"High contrast image\"), (low_contrast_image, \"Low contrast image\"), figsize = (7, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### Adjust saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def solution_adjust_saturation(img, saturation_factor):\n",
    "    # Convert the image from BGR to HSV\n",
    "    image_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Split the HSV image into Hue, Saturation, and Value channels\n",
    "    hue, saturation, value = cv2.split(image_hsv)\n",
    "\n",
    "    # Adjust the saturation channel (Ensure it stays within valid range)\n",
    "    saturation = np.clip(saturation * saturation_factor, 0, 255)\n",
    "\n",
    "    # Merge the channels back\n",
    "    image_hsv_adjusted = cv2.merge([hue, saturation.astype(np.uint8), value])\n",
    "\n",
    "    # Convert the adjusted image back to BGR\n",
    "    return cv2.cvtColor(image_hsv_adjusted, cv2.COLOR_HSV2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease saturation\n",
    "low_saturation_image = solution_adjust_saturation(image, 0.2)\n",
    "\n",
    "# Increase saturation\n",
    "high_saturation_image = solution_adjust_saturation(image, 2.5)\n",
    "\n",
    "if low_saturation_image is not None and high_saturation_image is not None:\n",
    "    # Use this function to plot images side by side\n",
    "    plot_multiple_images((image, \"Original\"), (low_saturation_image, \"Low saturation image\"), (high_saturation_image, \"High saturation image\"), figsize = (7, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## Image classifier development using CNNs\n",
    "\n",
    "Image classification is the task of assigning a label or category to an input image from a predefined set of classes. It is a fundamental problem in computer vision with widespread applications, including facial recognition, medical imaging, quality control, and autonomous driving. This section outlines the key steps involved in developing an image classification model using PyTorch. It begins with data preprocessing, which includes splitting the dataset into training, validation, and test sets. Afterwards, it defines data augmentation strategies using the Albumentations library, loads the data as PyTorch datasets, and initialises PyTorch dataloaders to efficiently feed data during training. The next step is model training, where a CNN-based model is initialized and optimised using the training and validation data. After training, the model is evaluated on the test set to assess its performance. The evaluation includes metrics such as accuracy and the confusion matrix, which help interpret the model's predictive behavior. Finally, the PyTorch Grad-CAM library is used to visualize the regions of input images that contribute most to the model’s decisions, providing insights into model explainability using representative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Train, validation, and test sets\n",
    "\n",
    "```train_test_split``` from Scikit-learn can be used to split the original training set into training and validation sets. The test set is already defined by the dataset' authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation sets\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Test set\n",
    "X_test, y_test = test_set[0], test_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "\n",
    "Data augmentation is a crucial technique in image classification that helps improve the performance and robustness of machine learning models. It involves generating new training samples by applying random transformations — such as rotation, flipping, cropping, scaling, or color jittering — to the original images. Albumentations is one of the most widely used libraries for performing data augmentation in image classification tasks. It includes augmentation techniques that replicate operations commonly used in image processing, such as:\n",
    "- ```A.Affine``` for scaling;\n",
    "- ```A.Rotate``` for rotation;\n",
    "- ```A.HorizontalFlip``` for horizontal flipping;\n",
    "- ```A.VerticalFlip``` for vertical flipping;\n",
    "- ```A.ColorJitter``` for color jittering.\n",
    "\n",
    "Albumentations can also be used for image normalization (```A.Normalize```), resizing (```A.Resize```), and converting images to PyTorch tensors with the (Channel, Height, Width) format using ```A.ToTensorV2```, which is required for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = 32\n",
    "\n",
    "# Transformations performed on train set\n",
    "train_transform = A.Compose([\n",
    "    A.Affine(scale = (0.2, 1.5), p = 0.1),\n",
    "    A.Rotate(limit = 45, p = 0.1),\n",
    "    A.HorizontalFlip(p = 0.1),\n",
    "    A.VerticalFlip(p = 0.1),\n",
    "    A.ColorJitter(brightness = (0.5, 1.5), contrast = (0.5, 1.5), saturation = (0.5, 1.5), hue = (0,0), p = 0.1), # Applied only to 'image'\n",
    "    A.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616)), # Applied only to 'image'\n",
    "    A.Resize(height = TARGET_SIZE, width = TARGET_SIZE),\n",
    "    A.ToTensorV2()\n",
    "])\n",
    "\n",
    "# Transformations performed on validation and test sets\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616)), # Applied only to 'image'\n",
    "    A.Resize(height = TARGET_SIZE, width = TARGET_SIZE),\n",
    "    A.ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### PyTorch Datasets\n",
    "\n",
    "```ImageDataset``` class is based on PyTorch ```Dataset``` class and is used for loading the images and their corresponding labels, for applying transformations (such as data augmentation), and returns them in a format suitable for model training, validating, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset classes necessary for the data loaders\n",
    "train_dataset = ImageDataset(X_train, y_train, transform = train_transform)\n",
    "val_dataset = ImageDataset(X_val, y_val, transform = val_transform)\n",
    "test_dataset = ImageDataset(X_test, y_test, transform = val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "#### PyTorch Dataloaders\n",
    "\n",
    "```DataLoader``` is essential for training efficiency and performance. It abstracts the complexity of batching, shuffling, and parallel data access, allowing you to focus on building and training your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders needed for the model training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Model training comprises a series of steps. First, we must check which devices are available for training the model. In case a GPU with Cuda cores is available is should be used as it really improves the speed. Otherwise, lets use CPU. Then, model and training hyperparameters should be defined, such as numer of output classes, number of training epochs, number of consecutive not improving epochs needed for stopping the training in case we use early stopping regularisation, and learning rate. Other hyperparameters can be defined, it depends on what the user wants to do during the training. In this notebook we are going to define the number of epochs, which are the number of times the model is going to see the training set. Early stopping is a way of trying to avoid overfitting where the model evaluates the model every new epoch using a validation set. In case the loss obtained for the validation set does not decrease for a long period of time (pre-defined epochs), the model optimisation stops and retrieves the checkpoint where the validation loss got the last decrease (see [Early Stopping](https://paperswithcode.com/method/early-stopping)). Learning rate defined how quick the models weights should change during training. If it is too high the weights are going to change really quick and might miss minima because they are always jumping from one side to another side. If it is too small the model weights might get stuck a local minimum. So although this is not done in this notebook, this parameter should be studied in order to choose the best (see [What is learning rate in machine learning?](https://www.ibm.com/think/topics/learning-rate)). After defining the hyperparameters, we should define the loss function that is going to be used to evaluate the model and it should be sent to the hardware used for training. Afterwards, the model is defined using ```ImageClassifier``` class and is sent to the device used for training. Next, we should define the optimiser function and also send it to the device used for training. Afterwards, we train the model in case some optimised weights are not available and we explore the learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "#### Check which device is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### Define training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of output classes\n",
    "NUMBER_CLASSES = len(CIFAR_10_CLASSES)\n",
    "\n",
    "# Number of training epochs\n",
    "EPOCHS = 500\n",
    "\n",
    "# Number of consecutive not improving epochs needed for stopping the training\n",
    "EARLY_STOPPING_LIMIT = EPOCHS // 10\n",
    "\n",
    "# Learning rate\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "The cross entropy loss function is defined by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\mathcal{L}$: Cross-entropy loss\n",
    "\n",
    "$C$: Total number of classes\n",
    "\n",
    "$y_i$: Ground truth indicator for class $i$, where $y_i = 1$ if class $i$ is the correct class, otherwise $y_i = 0$\n",
    "\n",
    "$\\hat{y}_i$: Predicted probability for class $i$, typically from the softmax output, where $0 \\leq \\hat{y}_i \\leq 1$ and $\\sum_{i=1}^{C} \\hat{y}_i = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "#### Initialise model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise image classifier\n",
    "model = ImageClassifier(in_channels = 3, out_classes = NUMBER_CLASSES)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### Optimiser function\n",
    "\n",
    "In this notebook, we are using Adam optimiser which is one of the most used optimisers in deep neural network optimisation (see [Gentle Introduction to the Adam Optimisation Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "The parameter update at each step is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
    "\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\theta_t$: Parameters at time step $t$\n",
    "\n",
    "$g_t$: Gradient of the loss with respect to parameters at step $t$\n",
    "\n",
    "$m_t$: Exponentially decaying average of past gradients (1st moment)\n",
    "\n",
    "$v_t$: Exponentially decaying average of past squared gradients (2nd moment)\n",
    "\n",
    "$\\hat{m}_t$, $\\hat{v}_t$: Bias-corrected estimates of $m_t$ and $v_t$\n",
    "\n",
    "$\\alpha$: Learning rate\n",
    "\n",
    "$\\beta_1$: Decay rate for the first moment estimate (typically 0.9)\n",
    "\n",
    "$\\beta_2$: Decay rate for the second moment estimate (typically 0.999)\n",
    "\n",
    "$\\epsilon$: Small constant to prevent division by zero (e.g., 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Optimiser\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "\n",
    "Here, we train the model. First, we initialise the class ```Trainer``` which we are going to use for training and evaluating the model using the PyTorch ```Dataset```s defined before. In case, some model weights are already available, we can skip the training and using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the image classifier\n",
    "trainer = Trainer(model)\n",
    "model_path = \"cnn_weights.pt\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading model weights...\")\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"Training model weights...\")\n",
    "    # Fit model\n",
    "    trainer.fit(EPOCHS, train_dataloader, val_dataloader, optimizer, criterion, DEVICE, EARLY_STOPPING_LIMIT)\n",
    "    # Save model weights\n",
    "    torch.save(trainer.model.best_model_weights, \"cnn_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "#### Learning curves\n",
    "\n",
    "After training the model, we can analyse the learning curves to assess the training process. These curves, which typically display the loss over epochs for both the training and validation sets, are crucial for improving model performance. They can help identify issues like overfitting or underfitting. Overfitting occurs when the model performs well on the training data but poorly on the validation data, usually indicated by a widening gap between the two curves. Underfitting, on the other hand, is suggested when both the training and validation curves show poor performance and fail to improve. By monitoring these curves, we can adjust hyperparameters or modify the model architecture to address such issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log = pd.read_csv(\"training_log.txt\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_log[\"Train Loss\"])\n",
    "plt.plot(training_log[\"Val Loss\"])\n",
    "plt.legend([\"Train loss\", \"Val loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning curve of image classification model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### Model testing\n",
    "\n",
    "After having the model already optimised, we can evaluate the model using the ```Trainer``` class by calling the ```predict``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images, true_labels, predicted_labels = trainer.predict(test_dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "### Explore results\n",
    "\n",
    "To evaluate the results, we display the model's accuracy along with the confusion matrix. The confusion matrix is a powerful evaluation tool that helps us understand the model’s performance across multiple classes. It maps the relationship between true and predicted labels, showing the number of instances for each possible prediction-outcome pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Compute average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average accuracy\n",
    "num_test_samples = len(original_images)\n",
    "correct = (true_labels == predicted_labels).sum()\n",
    "print(\"Accuracy:\", correct/num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "#### Compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.matshow(cm, cmap='Greens')\n",
    "\n",
    "# Add labels, title, and ticks\n",
    "ax.set_xticks(np.arange(NUMBER_CLASSES))\n",
    "ax.set_yticks(np.arange(NUMBER_CLASSES))\n",
    "ax.set_xticklabels(CIFAR_10_CLASSES)\n",
    "ax.set_yticklabels(CIFAR_10_CLASSES)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix for test set of CIFAR10')\n",
    "\n",
    "# Annotate each cell with the numeric value\n",
    "for (i, j), val in np.ndenumerate(cm):\n",
    "    ax.text(j, i, f'{val}', ha='center', va='center', color='black')\n",
    "\n",
    "# Rotate class names on x-axis\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### Explain image classifier predictions\n",
    "\n",
    "Deep neural networks are often described as \"black boxes\" because their decision-making processes are difficult to understand and interpret. To address this, researchers have developed various methods to make these models more explainable. One such method is Grad-CAM (Gradient-weighted Class Activation Mapping). Grad-CAM computes the gradients of a target class with respect to the final convolutional layers and generates a heatmap that highlights the regions of the input image most influential in the model’s prediction for that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "#### Prepare image for GradCAM\n",
    "\n",
    "We begin by preparing the image for Grad-CAM visualisation. The image must be converted to the (Height, Width, Channels) format and normalized to values between 0 and 1, as the visualizer from the PyTorch-GradCAM library expects this format. Afterwards, a batch dimension should be added to make it compatible with the model's input requirements. We also retrieve the predicted and true labels, as both are needed for Grad-CAM computation and visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images\n",
    "idx = 1\n",
    "img = original_images[idx]\n",
    "img_np = np.transpose(img, (1,2,0)) # shape: (H, W, C)\n",
    "img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min()) # Should be normalised between 0 and 1\n",
    "img = np.expand_dims(img, axis = 0)\n",
    "img = torch.from_numpy(img)\n",
    "img = img.to(DEVICE)  # shape: [1, C, H, W]\n",
    "pred_label = predicted_labels[idx]\n",
    "true_label = true_labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "#### Compute GradCAM heatmap\n",
    "\n",
    "Using the predicted class, we compute the Grad-CAM values based on the activations and gradients from the last convolutional layer of the image classifier. This layer is typically chosen because it retains spatial information that helps localise the regions of the input image most relevant to the model's decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure input requires grad\n",
    "img.requires_grad = True\n",
    "\n",
    "# Define the layer(s) to inspect\n",
    "target_layers = [model.conv3]\n",
    "\n",
    "# Define the target class you want to explain\n",
    "targets = [ClassifierOutputTarget(pred_label)]\n",
    "\n",
    "# Make sure that the model is on eval and not on train mode\n",
    "model.eval()\n",
    "\n",
    "# Create CAM object\n",
    "with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "    grad_cam_matrix = cam(input_tensor=img, targets=targets)\n",
    "    grad_cam_matrix = grad_cam_matrix[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "#### Visualise GradCAM heatmap with the image\n",
    "\n",
    "After obtaining the Grad-CAM heatmap, we overlay it on the input image to visualise the regions that contributed most to the model’s prediction. This helps identify which pixels the model focused on when predicting the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CAM with image\n",
    "visualization = show_cam_on_image(img_np, grad_cam_matrix, use_rgb=True)\n",
    "\n",
    "# Plot image with GradCAM output\n",
    "true_class = CIFAR_10_CLASSES[true_label]\n",
    "pred_class = CIFAR_10_CLASSES[pred_label]\n",
    "plot_multiple_images((img_np, f\"Original - {true_class}\"), (visualization, f\"Grad-CAM - {pred_class}\"), figsize = (5,6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
